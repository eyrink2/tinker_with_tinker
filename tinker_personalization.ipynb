{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b87ac2",
   "metadata": {},
   "source": [
    "## HW2: Deep Personalization\n",
    "\n",
    "In the previous homework, you experimented with alignment methods including few-shot prompting, instruction tuning, RLHF, and DPO. Those experiments used general chat data to align a pre-trained model into a generally helpful assistant.\n",
    "\n",
    "However, helpfulness is often subjective. For example, while RLHF models tend to produce longer outputs, you may prefer short and concise responses when generating emails. In this assignment, you'll explore machine learning techniques to personalize model behavior to individual preferences.\n",
    "\n",
    "### Overview of LLM Personalization\n",
    "\n",
    "LLM Personalization encompasses the methodical application of machine learning techniques to customize large language model behavior based on user-specific data.\n",
    "\n",
    "<img src=\"assets/overview.jpg\" alt=\"Overview\" width=\"800\">\n",
    "\n",
    "A classic methodology is supervised fine-tuning (SFT) from CS224N—remember the sonnet generation assignment? 🙂 This is a widely used method especially before ChatGPT.\n",
    "\n",
    "<img src=\"assets/sft.jpg\" alt=\"SFT\" width=\"800\">\n",
    "\n",
    "The problem of the SFT method is that it requires the user to label desired outputs. This process is not only time-consuming but often infeasible for non-technical users. In this assignment, you'll explore alternative personalization methods that reduce this human effort barrier. Through hands-on experimentation, you'll analyze the pros and cons of different approaches.\n",
    "\n",
    "You will use **Tinker**, the cutting-edge training API developed by Thinking Machine Labs for this assignment. Besides following this handout, **we highly recommend checking out [Tinker Cookbook](https://tinker-docs.thinkingmachines.ai/) to understand Tinker abstraction**.\n",
    "\n",
    "**Note that Tinker handles the heavy computation for forward and backward passes. As a result, the remaining code can run on your laptop even during model training.**\n",
    "\n",
    "You also need to use [wandb](https://wandb.ai/site) to monitor the training process. Create an account if you don't have one yet, and make sure to add your API key to the `.env` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66a3c0",
   "metadata": {},
   "source": [
    "### Setup & Data Collection\n",
    "\n",
    "1. **Install Tinker**: Run the following cells to install the Tinker API.\n",
    "\n",
    "2. **Collect your personalized data**: Make a copy of `data.example.json` and rename this file to `data.json`. Add 10 data points in the format `{\"input\": \"...\", \"output\": \"...\"}` for email generation. The outputs should come from your own emails to better reflect your personal writing style.\n",
    "  \n",
    "    **Recall the lecture on \"Data, Data, and Data\", the similar logic applies here. Please ensure the emails you choose are representative to your style and cover diverse topic. Ensure the \"input\" instruction provides all necessary context (e.g., name, intent), similar to your request to LLM when you want to use LLM to get your things done.**\n",
    "   \n",
    "   - **Bonus (20 points):** While we expect most students to work on the email generation task, we encourage you to explore creative data sources for personalization. If you choose to collect `{\"input\": \"...\", \"output\": \"...\"}` pairs from a different domain (not email generation), you may need to modify some parts of the provided code below.\n",
    "   \n",
    "   - **If you're attempting the bonus**, describe your data source here:\n",
    "     \n",
    "     **TODO: Add your answer**\n",
    "\n",
    "Hint: You may want to use a programmatic approach to convert inputs and outputs into the required JSON format, which will help you avoid the hassle of dealing with \\n escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84aca901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in ./.conda/lib/python3.11/site-packages (0.22.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /Users/eyrinkim/.local/lib/python3.11/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in ./.conda/lib/python3.11/site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from wandb) (2.12.3)\n",
      "Requirement already satisfied: pyyaml in ./.conda/lib/python3.11/site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.conda/lib/python3.11/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./.conda/lib/python3.11/site-packages (from wandb) (2.42.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic<3->wandb) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tinker in ./.conda/lib/python3.11/site-packages (0.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from httpx[http2]<1,>=0.23.0->tinker) (0.28.1)\n",
      "Requirement already satisfied: numpy in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker) (2.3.4)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker) (1.3.1)\n",
      "Requirement already satisfied: torch in ./.conda/lib/python3.11/site-packages (from tinker) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->tinker) (3.11)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/eyrinkim/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in ./.conda/lib/python3.11/site-packages (from httpx[http2]<1,>=0.23.0->tinker) (4.3.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in ./.conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in ./.conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->tinker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->tinker) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->tinker) (0.4.2)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from torch->tinker) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.conda/lib/python3.11/site-packages (from torch->tinker) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from torch->tinker) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.11/site-packages (from torch->tinker) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.conda/lib/python3.11/site-packages (from torch->tinker) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch->tinker) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2->torch->tinker) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/thinking-machines-lab/tinker-cookbook.git\n",
      "  Cloning https://github.com/thinking-machines-lab/tinker-cookbook.git to /private/var/folders/c2/dmtmvmwj4r3670ctrstvcjh40000gn/T/pip-req-build-vzsonu5k\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/thinking-machines-lab/tinker-cookbook.git /private/var/folders/c2/dmtmvmwj4r3670ctrstvcjh40000gn/T/pip-req-build-vzsonu5k\n",
      "  Resolved https://github.com/thinking-machines-lab/tinker-cookbook.git to commit 3dd0463472dda5847efee80010b50514fa3068ef\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: blobfile in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: chz in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (0.3.0)\n",
      "Requirement already satisfied: datasets in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (4.3.0)\n",
      "Requirement already satisfied: inspect-ai in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (0.3.140)\n",
      "Requirement already satisfied: math-verify in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (0.8.0)\n",
      "Requirement already satisfied: numpy in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (2.3.4)\n",
      "Requirement already satisfied: pylatexenc in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (2.10)\n",
      "Requirement already satisfied: rich in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (14.2.0)\n",
      "Requirement already satisfied: scipy in /Users/eyrinkim/.local/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (1.15.3)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: termcolor in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: textarena in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (0.7.4)\n",
      "Requirement already satisfied: tinker in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (0.2.2)\n",
      "Requirement already satisfied: torch in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (2.9.0)\n",
      "Requirement already satisfied: transformers in ./.conda/lib/python3.11/site-packages (from tinker_cookbook==0.1.0) (4.57.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.11/site-packages (from anyio->tinker_cookbook==0.1.0) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from anyio->tinker_cookbook==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from anyio->tinker_cookbook==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in ./.conda/lib/python3.11/site-packages (from blobfile->tinker_cookbook==0.1.0) (3.23.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in ./.conda/lib/python3.11/site-packages (from blobfile->tinker_cookbook==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.9 in ./.conda/lib/python3.11/site-packages (from blobfile->tinker_cookbook==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: filelock>=3.0 in ./.conda/lib/python3.11/site-packages (from blobfile->tinker_cookbook==0.1.0) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.conda/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/eyrinkim/.local/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.conda/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.conda/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.conda/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in ./.conda/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./.conda/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /Users/eyrinkim/.local/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.11/site-packages (from datasets->tinker_cookbook==0.1.0) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.conda/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (3.13.1)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets->tinker_cookbook==0.1.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/eyrinkim/.local/lib/python3.11/site-packages (from httpx<1.0.0->datasets->tinker_cookbook==0.1.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->tinker_cookbook==0.1.0) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->tinker_cookbook==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->tinker_cookbook==0.1.0) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets->tinker_cookbook==0.1.0) (3.4.4)\n",
      "Requirement already satisfied: aioboto3>=13.0.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (15.4.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (4.14.2)\n",
      "Requirement already satisfied: boto3 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.40.49)\n",
      "Requirement already satisfied: click!=8.2.0,>=8.1.3 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (8.3.0)\n",
      "Requirement already satisfied: debugpy in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.8.16)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (0.17.0)\n",
      "Requirement already satisfied: ijson>=3.2.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (3.4.0.post0)\n",
      "Requirement already satisfied: jsonlines>=3.0.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: jsonpatch>=1.32 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.33)\n",
      "Requirement already satisfied: jsonpath-ng>=1.7.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.7.0)\n",
      "Requirement already satisfied: jsonref>=1.1.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: jsonschema>3.1.1 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (4.25.1)\n",
      "Requirement already satisfied: mmh3>3.1.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (5.2.0)\n",
      "Requirement already satisfied: nest_asyncio in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: platformdirs>=2.3.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (4.5.0)\n",
      "Requirement already satisfied: psutil in /Users/eyrinkim/.local/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (7.0.0)\n",
      "Requirement already satisfied: pydantic>=2.11.4 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (2.12.3)\n",
      "Requirement already satisfied: python-dotenv>=0.16.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: s3fs>=2023 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (2025.9.0)\n",
      "Requirement already satisfied: semver>=3.0.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (3.0.4)\n",
      "Requirement already satisfied: shortuuid in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (1.0.13)\n",
      "Requirement already satisfied: tenacity in /Users/eyrinkim/.local/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (9.1.2)\n",
      "Requirement already satisfied: textual>=2.1.0 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (6.4.0)\n",
      "Requirement already satisfied: universal-pathlib>=0.2.6 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (0.3.4)\n",
      "Requirement already satisfied: zipp>=3.19.1 in ./.conda/lib/python3.11/site-packages (from inspect-ai->tinker_cookbook==0.1.0) (3.23.0)\n",
      "Requirement already satisfied: aiobotocore==2.25.0 in ./.conda/lib/python3.11/site-packages (from aiobotocore[boto3]==2.25.0->aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (2.25.0)\n",
      "Requirement already satisfied: aiofiles>=23.2.1 in ./.conda/lib/python3.11/site-packages (from aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (25.1.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in ./.conda/lib/python3.11/site-packages (from aiobotocore==2.25.0->aiobotocore[boto3]==2.25.0->aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.40.50,>=1.40.46 in ./.conda/lib/python3.11/site-packages (from aiobotocore==2.25.0->aiobotocore[boto3]==2.25.0->aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (1.40.49)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from aiobotocore==2.25.0->aiobotocore[boto3]==2.25.0->aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.conda/lib/python3.11/site-packages (from aiobotocore==2.25.0->aiobotocore[boto3]==2.25.0->aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in ./.conda/lib/python3.11/site-packages (from aiobotocore==2.25.0->aiobotocore[boto3]==2.25.0->aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (1.17.3)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in ./.conda/lib/python3.11/site-packages (from boto3->inspect-ai->tinker_cookbook==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore==2.25.0->aiobotocore[boto3]==2.25.0->aioboto3>=13.0.0->inspect-ai->tinker_cookbook==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.conda/lib/python3.11/site-packages (from beautifulsoup4>=4.10.0->inspect-ai->tinker_cookbook==0.1.0) (2.8)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch>=1.32->inspect-ai->tinker_cookbook==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: ply in ./.conda/lib/python3.11/site-packages (from jsonpath-ng>=1.7.0->inspect-ai->tinker_cookbook==0.1.0) (3.11)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.conda/lib/python3.11/site-packages (from jsonschema>3.1.1->inspect-ai->tinker_cookbook==0.1.0) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.conda/lib/python3.11/site-packages (from jsonschema>3.1.1->inspect-ai->tinker_cookbook==0.1.0) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.conda/lib/python3.11/site-packages (from jsonschema>3.1.1->inspect-ai->tinker_cookbook==0.1.0) (0.28.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic>=2.11.4->inspect-ai->tinker_cookbook==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic>=2.11.4->inspect-ai->tinker_cookbook==0.1.0) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pydantic>=2.11.4->inspect-ai->tinker_cookbook==0.1.0) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/lib/python3.11/site-packages (from rich->tinker_cookbook==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.11/site-packages (from rich->tinker_cookbook==0.1.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->tinker_cookbook==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: mdit-py-plugins in ./.conda/lib/python3.11/site-packages (from textual>=2.1.0->inspect-ai->tinker_cookbook==0.1.0) (0.5.0)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in ./.conda/lib/python3.11/site-packages (from markdown-it-py[linkify]>=2.1.0->textual>=2.1.0->inspect-ai->tinker_cookbook==0.1.0) (2.0.3)\n",
      "Requirement already satisfied: uc-micro-py in ./.conda/lib/python3.11/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.1.0->textual>=2.1.0->inspect-ai->tinker_cookbook==0.1.0) (1.0.3)\n",
      "Requirement already satisfied: pathlib-abc<0.6.0,>=0.5.1 in ./.conda/lib/python3.11/site-packages (from universal-pathlib>=0.2.6->inspect-ai->tinker_cookbook==0.1.0) (0.5.2)\n",
      "Requirement already satisfied: latex2sympy2_extended==1.10.2 in ./.conda/lib/python3.11/site-packages (from math-verify->tinker_cookbook==0.1.0) (1.10.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime<=4.13.2,>=4.9.3 in ./.conda/lib/python3.11/site-packages (from latex2sympy2_extended==1.10.2->math-verify->tinker_cookbook==0.1.0) (4.13.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pandas->datasets->tinker_cookbook==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from pandas->datasets->tinker_cookbook==0.1.0) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.11/site-packages (from sympy->tinker_cookbook==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: openai in /Users/eyrinkim/.local/lib/python3.11/site-packages (from textarena->tinker_cookbook==0.1.0) (2.6.0)\n",
      "Requirement already satisfied: nltk in ./.conda/lib/python3.11/site-packages (from textarena->tinker_cookbook==0.1.0) (3.9.2)\n",
      "Requirement already satisfied: chess in ./.conda/lib/python3.11/site-packages (from textarena->tinker_cookbook==0.1.0) (1.11.2)\n",
      "Requirement already satisfied: websockets in ./.conda/lib/python3.11/site-packages (from textarena->tinker_cookbook==0.1.0) (15.0.1)\n",
      "Requirement already satisfied: joblib in /Users/eyrinkim/.local/lib/python3.11/site-packages (from nltk->textarena->tinker_cookbook==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.conda/lib/python3.11/site-packages (from nltk->textarena->tinker_cookbook==0.1.0) (2025.10.23)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from openai->textarena->tinker_cookbook==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from openai->textarena->tinker_cookbook==0.1.0) (0.11.1)\n",
      "Requirement already satisfied: h2<5,>=3 in ./.conda/lib/python3.11/site-packages (from httpx[http2]<1,>=0.23.0->tinker->tinker_cookbook==0.1.0) (4.3.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in ./.conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker->tinker_cookbook==0.1.0) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in ./.conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker->tinker_cookbook==0.1.0) (4.1.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/eyrinkim/.local/lib/python3.11/site-packages (from torch->tinker_cookbook==0.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.11/site-packages (from torch->tinker_cookbook==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2->torch->tinker_cookbook==0.1.0) (3.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.conda/lib/python3.11/site-packages (from transformers->tinker_cookbook==0.1.0) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.conda/lib/python3.11/site-packages (from transformers->tinker_cookbook==0.1.0) (0.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%pip install tinker\n",
    "%pip install git+https://github.com/thinking-machines-lab/tinker-cookbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97c9e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import tinker\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc40523e-edaa-4a5a-beea-41de12716491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- deepseek-ai/DeepSeek-V3.1\n",
      "- deepseek-ai/DeepSeek-V3.1-Base\n",
      "- meta-llama/Llama-3.1-70B\n",
      "- meta-llama/Llama-3.1-8B\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.2-1B\n",
      "- meta-llama/Llama-3.2-3B\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "- Qwen/Qwen3-30B-A3B\n",
      "- Qwen/Qwen3-30B-A3B-Base\n",
      "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "- Qwen/Qwen3-32B\n",
      "- Qwen/Qwen3-4B-Instruct-2507\n",
      "- Qwen/Qwen3-8B\n",
      "- Qwen/Qwen3-8B-Base\n",
      "- openai/gpt-oss-120b\n",
      "- openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "service_client = tinker.ServiceClient()\n",
    "print(\"Available models:\")\n",
    "for item in service_client.get_server_capabilities().supported_models:\n",
    "    print(\"- \" + item.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c880275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 demonstrations loaded.\n"
     ]
    }
   ],
   "source": [
    "demonstrations = json.load(open(\"data.json\"))\n",
    "print(len(demonstrations), \"demonstrations loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb379b",
   "metadata": {},
   "source": [
    "### Get Baseline Results (10 points total)\n",
    "\n",
    "**Implement inference code (10 points)**\n",
    "\n",
    "In the following experiments, we will use `Qwen/Qwen3-4B-Instruct-2507` as the baseline policy. After finishing the assignment, we highly recommend trying out larger models available on Tinker.\n",
    "\n",
    "Use the inference code to obtain baseline results by prompting `Qwen/Qwen3-4B-Instruct-2507` with your inputs directly. `Qwen/Qwen3-4B-Instruct-2507` is a model that has been aligned with general human feedback. Examine its outputs to see whether they satisfy your personal preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4ec43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinker import types\n",
    "from tinker_cookbook import renderers\n",
    "from tinker_cookbook.model_info import get_recommended_renderer_name\n",
    "from tinker_cookbook.tokenizer_utils import get_tokenizer\n",
    "\n",
    "\n",
    "class TinkerSampler():\n",
    "    \"\"\"A simple wrapper around Tinker ServiceClient to do sampling.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_path: str | None = None,  # tinker://..., obtained from Tinker training job\n",
    "        temperature: float = 0.9,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        top_k=-1,  # -1 means no limit\n",
    "    ):\n",
    "        tokenizer = get_tokenizer(model_name)\n",
    "        renderer_name = get_recommended_renderer_name(model_name)\n",
    "        # Read https://tinker-docs.thinkingmachines.ai/rendering to understand what renderer is\n",
    "        self.renderer = renderers.get_renderer(name=renderer_name, tokenizer=tokenizer)\n",
    "        self.sampling_params = types.SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=self.renderer.get_stop_sequences(),\n",
    "        )\n",
    "        self.sampling_client = service_client.create_sampling_client(\n",
    "            model_path=model_path,\n",
    "            base_model=model_name,\n",
    "        )\n",
    "        \n",
    "    async def generate(self, messages: list[renderers.Message]) -> renderers.Message:\n",
    "        # convert messages to the format expected by the sampling client\n",
    "        generation_prompt = self.renderer.build_generation_prompt(messages)\n",
    "        \n",
    "        # sample from the model, sample_async is async so need await\n",
    "        response = await self.sampling_client.sample_async(\n",
    "            generation_prompt,\n",
    "            num_samples=1,\n",
    "            sampling_params=self.sampling_params,\n",
    "        )\n",
    "        \n",
    "        # decode the response tokens to get the text\n",
    "        generated_text = self.renderer.tokenizer.decode(response.sequences[0].tokens) # we only request one sample\n",
    "        \n",
    "        # return as a message object\n",
    "        return renderers.Message(role=\"assistant\", content=generated_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30445938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1/10\n",
      "Sampling 2/10\n",
      "Sampling 3/10\n",
      "Sampling 4/10\n",
      "Sampling 5/10\n",
      "Sampling 6/10\n",
      "Sampling 7/10\n",
      "Sampling 8/10\n",
      "Sampling 9/10\n",
      "Sampling 10/10\n",
      "Input:  Write an email asking a colleague to make an introduction to Lumen, an energy finance company, to invite them to speak on a fireside chat event on green financethat I am hosting for Stanford Climate Week. Include information about Stanford Climate Week and the event details.\n",
      "==================================================\n",
      "Expected Output:  Hi Shawn,\n",
      "\n",
      " Stanford Climate Week is hosting a fireside chat on reimagining climate finance during the week of October 20th. We'd love to invite Lumen to join alongside Crux's CEO (who has confirmed) to discuss the future of funding green solutions. If there’s interest and availability, please let me know. More about Stanford Climate Week: https://www.stanfordclimateweek.com/\n",
      "\n",
      " Thanks for your help, and let me know if I can answer any questions!\n",
      "\n",
      " Warmly, Eyrin\n",
      "==================================================\n",
      "Output:  Subject: Invitation to Speak on Fireside Chat: Green Finance at Stanford Climate Week  \n",
      "\n",
      "Hi [Colleague's Name],  \n",
      "\n",
      "I hope you're doing well! I’m reaching out because I’ve been thinking about the upcoming Stanford Climate Week, and I’d love for you to consider joining me as a speaker on one of the key events: a fireside chat focused on green finance.  \n",
      "\n",
      "Stanford Climate Week is a premier gathering of climate leaders, investors, policymakers, and innovators from around the world. The event brings together experts to explore cutting-edge solutions for decarbonization, climate resilience, and sustainable economic transformation. This year’s theme centers on creating a just and equitable transition through innovative financing—making green finance not just a pathway to sustainability, but a powerful engine of economic opportunity.  \n",
      "\n",
      "I’m organizing a fireside chat titled *“Powering the Green Transition: The Role of Finance in Building a Sustainable Future”* on [insert date, e.g., Thursday, September 26], at [insert time, e.g., 11:00 AM PT]. The event will take place in [insert location, e.g., Stanford’s Davies Hall] and will be open to a live audience of students, faculty, industry professionals, and climate advocates. The format is conversational and interactive—perfect for thought leaders like yourself who can share real-world insights.  \n",
      "\n",
      "Given your experience at Lumen, an innovative energy finance company driving tangible climate impact through strategic financial instruments, your perspective would be incredibly valuable to the audience. I’d be honored if you’d consider joining us as a guest speaker.  \n",
      "\n",
      "Would you be open to a brief call next week to discuss the event further, share your thoughts on how Lumen’s work aligns with this theme, and go over logistics? I’d be happy to accommodate your schedule.  \n",
      "\n",
      "Thank you so much for considering this—your voice would make a meaningful difference in shaping the conversation around green finance at Stanford.  \n",
      "\n",
      "Looking forward to hearing from you!  \n",
      "\n",
      "Best regards,  \n",
      "[Your Full Name]  \n",
      "[Your Position/Title]  \n",
      "[Your Department/Institution, e.g., Climate Initiative Lead, Stanford Graduate School of Business]  \n",
      "[Email Address] | [Phone Number, optional]  \n",
      "Event Website: [link, if available]  \n",
      "\n",
      "P.S. Stanford Climate Week is free and open to all—this fireside chat will be recorded and shared with participants and a wider audience, offering a great opportunity to amplify your work.  \n",
      "\n",
      "—  \n",
      "*Note: Be sure to personalize the bracketed details (date, time, location, etc.) before sending.*<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "baseline_sampler = TinkerSampler(\n",
    "    model_name=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    ")\n",
    "\n",
    "baseline_results = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [renderers.Message(role=\"user\", content=input_text)]\n",
    "    output = await baseline_sampler.generate(messages)\n",
    "    baseline_results.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", baseline_results[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", baseline_results[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", baseline_results[0][\"output\"])\n",
    "\n",
    "# The cell only prints the first example. Read through all baseline results in results/baseline_results.json\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/baseline_results.json\", \"w\") as f:\n",
    "    json.dump(baseline_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8637c22",
   "metadata": {},
   "source": [
    "### Method 1: Prompt Engineering (10 points total)\n",
    "\n",
    "<img src=\"assets/prompting.jpg\" alt=\"Prompting\" width=\"800\">\n",
    "\n",
    "Prompt engineering requires no model training. Instead, you craft instructions that guide the model to produce outputs matching your preferences by describing your desired behavior or providing in-context examples.\n",
    "\n",
    "If you need to refresh your understanding of prompt engineering techniques (such as few-shot examples, chain-of-thought prompting, etc.), we recommend revisiting HW1.\n",
    "\n",
    "**Engineer a system prompt (5 points)**\n",
    "\n",
    "Craft a system prompt that guides the model to generate emails in your personal style (or for your own selected task). Experiment with different prompt strategies to see which produces the most personalized outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa86e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1/10\n",
      "Sampling 2/10\n",
      "Sampling 3/10\n",
      "Sampling 4/10\n",
      "Sampling 5/10\n",
      "Sampling 6/10\n",
      "Sampling 7/10\n",
      "Sampling 8/10\n",
      "Sampling 9/10\n",
      "Sampling 10/10\n",
      "Input:  Write an email asking a colleague to make an introduction to Lumen, an energy finance company, to invite them to speak on a fireside chat event on green financethat I am hosting for Stanford Climate Week. Include information about Stanford Climate Week and the event details.\n",
      "==================================================\n",
      "Expected Output:  Hi Shawn,\n",
      "\n",
      " Stanford Climate Week is hosting a fireside chat on reimagining climate finance during the week of October 20th. We'd love to invite Lumen to join alongside Crux's CEO (who has confirmed) to discuss the future of funding green solutions. If there’s interest and availability, please let me know. More about Stanford Climate Week: https://www.stanfordclimateweek.com/\n",
      "\n",
      " Thanks for your help, and let me know if I can answer any questions!\n",
      "\n",
      " Warmly, Eyrin\n",
      "==================================================\n",
      "Output:  Hi [Colleague's Name],\n",
      "\n",
      "I hope you're doing well. I'm reaching out because I’d love to invite you to speak on a fireside chat event I’m hosting as part of Stanford Climate Week. The event will focus on green finance and how innovative financial models are shaping the transition to a low-carbon future.\n",
      "\n",
      "Stanford Climate Week brings together students, faculty, and professionals to explore urgent climate challenges and solutions. This fireside chat will feature thought leaders discussing the role of finance in advancing clean energy and climate resilience. The event is open to the Stanford community and will be streamed on campus and online.\n",
      "\n",
      "Lumen, an energy finance company, is sponsoring the event, and I’d be honored to have you join as a speaker. Your perspective would add valuable depth to the discussion, especially given your experience in energy and finance.\n",
      "\n",
      "Would you be open to a short conversation this week to discuss the possibility? I’d be happy to provide more details or set up a time that works for you.\n",
      "\n",
      "Warmly,  \n",
      "Eyrin<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that writes emails. You will write them in a professional and friendly tone with no emdashes (--). You will limit exclamation marks and focus \n",
    "on being succinct while still polite. You are writing from the perspective of Eyrin Kim, a Stanford undergraduate student.\n",
    "\n",
    "Here are some examples of emails you have written (input was context and output was the email you wrote):\n",
    "\n",
    "        \"input\": \"Reply to an interviewer with times I am available this week to chat, broken down day by day. I work in pacific time.\",\n",
    "        \"output\": \"Hi Matias, It's great to meet you. I'd love to chat next week — do any of these times work for you? All times in PT:\\n\\n Tuesday, Oct 14: 10 am - 12 pm, 2 - 3 pm\\n\\n Wednesday, Oct 15: 4:30 - 6 pm\\n\\n Thursday, Oct 16: 10 am - 12 pm, 1 - 3 pm\\n\\n Friday, Oct 17: 1:30 - 4:30 pm\\n\\n Looking forward to connecting! Let me know if I can answer any questions in the meantime; I'm happy to send more info about myself.\\n\\n Warmly, Eyrin\"\n",
    "\n",
    "        \"input\": \"Write an email reaching out to a professional in the field of eningeering and energy working at Tapestry asking for an informational coffee chat. Mention my deep interest in the intersection of AI and energy, energy as a critical problem in America, and the work of Google X.\",\n",
    "        \"output\": \"Hi Arianna,\\n\\n My name is Eyrin, and I'm a Stanford junior studying AI and Earth Systems. I'm reaching out because I would love to learn more about what you do at Google X.\\n\\nI've been deeply interested in America's electric grid recently and stumbled across Tapestry. Further research led me to Bellwether and Chorus, both of which were directionally similar to work I've done and intellectually similar to the type of work I love (attaching my resume for more context). Frankly speaking, I think Google X is home to some of the coolest frontier energy/climate related innovation happening at the moment, and I'm curious to know more about how you interface with the labs as well as your journey to Google X.\\n\\n Would you be open to a 20-minute chat in the next few weeks? I'd be grateful for the chance to pick your brain.\\n\\n Warmly, Eyrin\"\n",
    "\n",
    "        \"input\": \"Write an email connecting an engineer at Palantir, Ganesh, who has simulation field research experience, to a founder of a startup in the field of solving the robotics sim2real gap, Bilal.\"\n",
    "        \"output\": \"Hi Bilal, Ganesh;\\n Ganesh, meet Bilal, a master's student at ETHZ working on some awesome technology to solve the sim2real gap for robotics. He's currently building his company in SF and interested in meeting sharp people knowledgeable about this space.\\n\\n Bilal, meet Ganesh, currently an FDE at Palantir. Ganesh has formerly done research the simulation field at Motional, NASA, and Shield AI.\\n\\n You both have context, so will leave you to connect.\\n\\n Warmly,\\nEyrin\"\n",
    "  \n",
    "Clear rules for writing emails:\n",
    "- Always sign off with \"Warmly, Eyrin\". Include a line break after \"Warmly,\".\n",
    "- Focus on being succinct while still polite. Use complete sentences and prose -- no bullet points.\n",
    "- For long emails, break the emails into appropirate sections to not overwhelm the reader with text. If there are three sentences or less, keep it as a single paragraph.\n",
    "- Always maintain professional tone, even with friends or more casual contacts.\n",
    "- Eyrin's intellectual interests are in AI, climate, and energy. Only incorporate these topics if it makes sense to do so given the input.\n",
    "- DO NOT, UNDER ANY CIRCUMSTANCES, USE EMDAHSES (--). ALWAYS OPT FOR PERIODS OR SEMICOLONS INSTEAD.\n",
    "\"\"\"\n",
    "\n",
    "# Explicitly require the model to only output the answer without any extra text\n",
    "system_prompt += \"\\n\\nMake sure to follow the instructions carefully and do not output anything else (such as \\\"Sure! Here's ...\\\", \\\"If you want ...\\\").\"\n",
    "\n",
    "prompt_engineering_results = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [\n",
    "        renderers.Message(role=\"system\", content=system_prompt),\n",
    "        renderers.Message(role=\"user\", content=input_text),\n",
    "    ]\n",
    "    output = await baseline_sampler.generate(messages)\n",
    "    prompt_engineering_results.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", prompt_engineering_results[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", prompt_engineering_results[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", prompt_engineering_results[0][\"output\"])\n",
    "\n",
    "# The cell only prints the first example. Read through all prompt engineering results in results/prompt_engineering_results.json\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/prompt_engineering_results.json\", \"w\") as f:\n",
    "    json.dump(prompt_engineering_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc7184",
   "metadata": {},
   "source": [
    "**Analyze the Pros & Cons of Personalization with Prompting. (5 points)**\n",
    "\n",
    "Give your answer in writeup.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178ba41",
   "metadata": {},
   "source": [
    "### Method 2: SFT with Synthetic Data (20 points total)\n",
    "\n",
    "<img src=\"assets/sft_with_synthetic_data.jpg\" alt=\"SFT with Synthetic Data\" width=\"800\">\n",
    "\n",
    "We can leverage more powerful models to synthesize training data for smaller models. This approach allows us to transfer the capabilities of large models into smaller, more efficient models without requiring manually labeled data. Even for your engineered prompt, larger models usually follow it better and they can generate high-quality outputs that serve as training targets for personalizing smaller models.\n",
    "\n",
    "**Step 1 - Synthesize inputs (5 points):** Use the provided code snippet to synthesize 100 input prompts similar to those in your collected data. Carefully review the quality of the generated prompts and adjust the synthesis parameters as needed before proceeding.\n",
    "\n",
    "*Note: If you're using a data source different from email generation (bonus track), you'll need to modify the code snippet accordingly.*\n",
    "\n",
    "**Step 2 - Synthesize outputs:** Now use the system prompt you engineered in Method 1 and a large LLM (e.g., `Qwen/Qwen3-235B-A22B-Instruct-2507`) to generate synthetic outputs for these input prompts. If the synthetic output quality is inadequate, consider implementing advanced techniques such as [chain-of-thought prompting](https://www.promptingguide.ai/techniques/cot), [self-critique](https://arxiv.org/abs/2305.11738), or other approaches that allocate more test-time compute to improve quality.\n",
    "\n",
    "Before proceeding, carefully review several samples to check data quality. Address any systematic issues you identify.\n",
    "\n",
    "**Step 3 - Train via SFT (15 points):** Complete `sft.py` to fine-tune `Qwen/Qwen3-4B-Instruct-2507` using supervised fine-tuning on your synthesized dataset.\n",
    "\n",
    "**Step 4 - Evaluate the checkpoint:** You may be surprised by how quickly training completes! Note that the model you trained is significantly smaller than the model used for data synthesis. How well can this smaller model generate personalized outputs? Use your inference code to sample outputs from the SFT checkpoint and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "036b87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting synthetic inputs for seed: research project communication\n",
      "Collecting synthetic inputs for seed: turn down request\n",
      "Collecting synthetic inputs for seed: open source outreach\n",
      "Collecting synthetic inputs for seed: job & career context\n",
      "Collecting synthetic inputs for seed: social events\n",
      "Collecting synthetic inputs for seed: time-sensitive crisis communications\n",
      "Collecting synthetic inputs for seed: reimbursement request\n",
      "Collecting synthetic inputs for seed: communication with health care providers (e.g., dentist, insurance companies, etc.)\n",
      "Collecting synthetic inputs for seed: legal & compliance\n",
      "Collecting synthetic inputs for seed: cold emailing\n",
      "Collected 100 synthetic inputs\n",
      "Examples:\n",
      "Write an email to a research collaborator summarizing the progress made on a joint project this month, highlighting key findings, next steps, and requesting feedback on the current direction of the work.\n",
      "Draft a cold email to a sustainability officer at a major university, introducing a climate engagement tool you helped develop and asking if they would be open to piloting it with their campus sustainability team.\n"
     ]
    }
   ],
   "source": [
    "# Step 1 (Synthesize inputs)\n",
    "# You may need to change SEED and SYNTHESIZE_INPUT_PROMPT if you choose your own task rather than email writing.\n",
    "\n",
    "SEEDS = [\n",
    "    \"research project communication\",\n",
    "    \"turn down request\",\n",
    "    \"open source outreach\",\n",
    "    \"job & career context\",\n",
    "    \"social events\",\n",
    "    \"time-sensitive crisis communications\",\n",
    "    \"reimbursement request\",\n",
    "    \"communication with health care providers (e.g., dentist, insurance companies, etc.)\",\n",
    "    \"legal & compliance\",\n",
    "    \"cold emailing\"\n",
    "]\n",
    "\n",
    "SYNTHESIZE_INPUT_PROMPT = \"\"\"\\\n",
    "Generate 10 new email writing instruction prompts based on the provided examples and the given seed. Give your answer in JSON format as follows. Do not output any other text.\n",
    "\n",
    "Output format:\n",
    "```json\n",
    "{\n",
    "    \"prompt_1\": \"Prompt 1\",\n",
    "    \"prompt_2\": \"Prompt 2\",\n",
    "    \"prompt_3\": \"Prompt 3\",\n",
    "    \"prompt_4\": \"Prompt 4\",\n",
    "    \"prompt_5\": \"Prompt 5\",\n",
    "    \"prompt_6\": \"Prompt 6\",\n",
    "    \"prompt_7\": \"Prompt 7\",\n",
    "    \"prompt_8\": \"Prompt 8\",\n",
    "    \"prompt_9\": \"Prompt 9\",\n",
    "    \"prompt_10\": \"Prompt 10\"\n",
    "}\n",
    "```\n",
    "\n",
    "----\n",
    "Examples:\n",
    "\n",
    "<examples>\n",
    "\n",
    "Seed: <seed>\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "data_synthesis_sampler = TinkerSampler(\n",
    "    model_name=\"Qwen/Qwen3-235B-A22B-Instruct-2507\",  # Use a stronger model for data synthesis\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "def collect_synthetic_input(output_str: str):\n",
    "    s = output_str.split(\"Output:\")[-1].strip(\"<|endoftext|>\").strip()\n",
    "    regex=r\"```(?:[a-zA-Z0-9_+-]*\\n)?([\\s\\S]*?)```\"\n",
    "    result = re.search(regex, s)\n",
    "    if result:\n",
    "        s = result.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"No JSON found in the output\")\n",
    "    result = json.loads(s)\n",
    "    return [result[f\"prompt_{i}\"] for i in range(1, 11)]\n",
    "\n",
    "examples = \"\\n\".join([\n",
    "    f\"Input: {d['input']}\\nOutput: {d['output']}\" for d in demonstrations[:3]\n",
    "])  # Use first 3 demonstrations as examples\n",
    "\n",
    "synthetic_inputs = []\n",
    "for seed in SEEDS:\n",
    "    print(f\"Collecting synthetic inputs for seed: {seed}\")\n",
    "    for _ in range(MAX_RETRIES):\n",
    "        messages = [renderers.Message(\n",
    "            role=\"user\",\n",
    "            content=SYNTHESIZE_INPUT_PROMPT.replace(\"<examples>\", examples).replace(\"<seed>\", seed)\n",
    "        )]\n",
    "        output = await data_synthesis_sampler.generate(messages)\n",
    "        try:\n",
    "            synthetic_inputs.extend(collect_synthetic_input(str(output[\"content\"])))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting synthetic input: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Collected {len(synthetic_inputs)} synthetic inputs\")\n",
    "print(\"Examples:\")\n",
    "print(synthetic_inputs[0])\n",
    "print(synthetic_inputs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd569953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total successful pairs: 100\n",
      "Examples:\n",
      "Prompt: Write an email to a research collaborator summarizing the progress made on a joint project this month, highlighting key findings, next steps, and requesting feedback on the current direction of the work.\n",
      "Output: Hi Alex,\n",
      "\n",
      "I hope you're doing well. I wanted to share a quick update on our project this month and get your thoughts on where we're headed.\n",
      "\n",
      "We’ve made solid progress on the anomaly detection pipeline for satellite-derived methane data. The model’s precision has improved significantly after incorporating temporal smoothing and adjusting for regional albedo variations—false positives have dropped by about 40% compared to the initial version. We also validated the results against a subset of TROPOMI ground truth observations, and the recall remains strong at 88%.\n",
      "\n",
      "One interesting finding is that the model begins to underperform in high-aerosol regions, especially during fire season in the western US. This suggests we may need to integrate an aerosol correction layer or condition the model on CAMS data. I’ve started exploring a few lightweight architectures that could run in tandem.\n",
      "\n",
      "Next steps include running the updated pipeline on a full year of data for California and prepping a draft figure set for the AGU presentation. I’d also like to begin scoping a short manuscript if you think we’re ready to start shaping the narrative.\n",
      "\n",
      "Does this direction still feel aligned with your vision? I’d especially appreciate your input on whether we should prioritize robustness in challenging atmospheric conditions or focus more on scalability for broader deployment.\n",
      "\n",
      "Warmly,\n",
      "Eyrin<|im_\n",
      "\n",
      "\n",
      "Prompt: Draft a cold email to a sustainability officer at a major university, introducing a climate engagement tool you helped develop and asking if they would be open to piloting it with their campus sustainability team.\n",
      "Output: Hi Taylor,\n",
      "\n",
      "My name is Eyrin Kim, and I'm a junior at Stanford studying AI and Earth Systems. I'm reaching out to share a climate engagement tool I've helped develop that aims to strengthen community involvement in campus sustainability initiatives through personalized feedback and interactive challenges.\n",
      "\n",
      "We've piloted an early version with students and staff at Stanford, and the response has been encouraging—participants reported increased awareness of sustainable behaviors and greater confidence in contributing to institutional climate goals. The tool integrates seamlessly with existing campus platforms and is designed to be lightweight for administrative teams to manage.\n",
      "\n",
      "I'd love to learn more about the work you're doing at [University Name] and explore whether this might be a good fit for your team to pilot. Would you be open to a 20-minute conversation sometime in the coming weeks?\n",
      "\n",
      "Warmly,  \n",
      "Eyrin<|im_\n"
     ]
    }
   ],
   "source": [
    "# Step 2 (Synthesize outputs)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 30\n",
    "\n",
    "async def process_all_prompts_threadpool(synthetic_inputs):\n",
    "    def sync_process_prompt(prompt):\n",
    "        try:\n",
    "            messages = [\n",
    "                renderers.Message(role=\"system\", content=system_prompt),  # Add the system prompt you have engineered that guides the model to output in the desired style\n",
    "                renderers.Message(role=\"user\", content=prompt)\n",
    "            ]\n",
    "            output = asyncio.run(data_synthesis_sampler.generate(messages))\n",
    "            return (prompt, output[\"content\"].strip(\"<|endoftext|>\").strip())\n",
    "        except Exception as e:\n",
    "            return (prompt, f\"ERROR: {str(e)}\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:\n",
    "        futures = [executor.submit(sync_process_prompt, prompt) for prompt in synthetic_inputs]\n",
    "        \n",
    "        results = []\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "    \n",
    "    return results\n",
    "    \n",
    "synthetic_input_output_pairs = await process_all_prompts_threadpool(synthetic_inputs)\n",
    "\n",
    "print(f\"\\nTotal successful pairs: {len(synthetic_input_output_pairs)}\")\n",
    "print(\"Examples:\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[0][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[0][1]}\\n\\n\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[-1][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[-1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3eb50481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "Prompt: Write an email to a research collaborator summarizing the progress made on a joint project this month, highlighting key findings, next steps, and requesting feedback on the current direction of the work.\n",
      "Output: Hi Alex,\n",
      "\n",
      "I hope you're doing well. I wanted to share a quick update on our project this month and get your thoughts on where we're headed.\n",
      "\n",
      "We’ve made solid progress on the anomaly detection pipeline for satellite-derived methane data. The model’s precision has improved significantly after incorporating temporal smoothing and adjusting for regional albedo variations, false positives have dropped by about 40% compared to the initial version. We also validated the results against a subset of TROPOMI ground truth observations, and the recall remains strong at 88%.\n",
      "\n",
      "One interesting finding is that the model begins to underperform in high-aerosol regions, especially during fire season in the western US. This suggests we may need to integrate an aerosol correction layer or condition the model on CAMS data. I’ve started exploring a few lightweight architectures that could run in tandem.\n",
      "\n",
      "Next steps include running the updated pipeline on a full year of data for California and prepping a draft figure set for the AGU presentation. I’d also like to begin scoping a short manuscript if you think we’re ready to start shaping the narrative.\n",
      "\n",
      "Does this direction still feel aligned with your vision? I’d especially appreciate your input on whether we should prioritize robustness in challenging atmospheric conditions or focus more on scalability for broader deployment.\n",
      "\n",
      "Warmly,\n",
      "Eyrin\n",
      "\n",
      "\n",
      "Prompt: Draft a cold email to a sustainability officer at a major university, introducing a climate engagement tool you helped develop and asking if they would be open to piloting it with their campus sustainability team.\n",
      "Output: Hi Taylor,\n",
      "\n",
      "My name is Eyrin Kim, and I'm a junior at Stanford studying AI and Earth Systems. I'm reaching out to share a climate engagement tool I've helped develop that aims to strengthen community involvement in campus sustainability initiatives through personalized feedback and interactive challenges.\n",
      "\n",
      "We've piloted an early version with students and staff at Stanford, and the response has been encouraging, participants reported increased awareness of sustainable behaviors and greater confidence in contributing to institutional climate goals. The tool integrates seamlessly with existing campus platforms and is designed to be lightweight for administrative teams to manage.\n",
      "\n",
      "I'd love to learn more about the work you're doing at [University Name] and explore whether this might be a good fit for your team to pilot. Would you be open to a 20-minute conversation sometime in the coming weeks?\n",
      "\n",
      "Warmly,  \n",
      "Eyrin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix systematic issues in the synthetic data.\n",
    "# For example, if you find the model often outputs \"Sure! Here's ...\" at the beginning of the output, you can add code to remove that.\n",
    "# Or if you find the model forgets to include \"Best regards, [Your Name]\" at the end of the email, you can add code to append that.\n",
    "# Overall, it's a good practice to go through the data and improve its quality as you can.\n",
    "def fix_output(output: str):\n",
    "    import re\n",
    "    # remove all \"...\" in output\n",
    "    output = output.replace(\"...\", \"\")\n",
    "    \n",
    "    # remove \"<|im_\" from email signatures\n",
    "    output = re.sub(r'<\\|im_', '', output)\n",
    "    \n",
    "    # replace em dashes with commas, it makes it seem AI-generated\n",
    "    # handles various dash types: em dash (—), en dash (–), and double hyphen (--)\n",
    "    output = re.sub(r'\\s*[—–]\\s*', ', ', output)\n",
    "    output = re.sub(r'\\s*--\\s*', ', ', output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "synthetic_input_output_pairs = [(prompt, fix_output(output)) for prompt, output in synthetic_input_output_pairs]\n",
    "\n",
    "print(\"Examples:\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[0][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[0][1]}\\n\\n\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[-1][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[-1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280d651",
   "metadata": {},
   "source": [
    "**Step 3: SFT**\n",
    "\n",
    "Complete sft.py and train \"Qwen/Qwen3-4B-Instruct-2507\" with the synthetic data before you proceed to the next cell.\n",
    "\n",
    "Under the root directory, launch the training with `python -m scripts.sft {other arguments}`.\n",
    "\n",
    "The training takes a few minutes given we only synthesize a small number of training data. But you shall already be able to see the model behavior change!\n",
    "\n",
    "**Add the wandb link for your run in writeup.md (15 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8bb85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data saved to results/synthetic_personalized_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "synthetic_data_path = \"results/synthetic_personalized_data.jsonl\"\n",
    "with open(synthetic_data_path, \"w\") as f:\n",
    "    for prompt, output in synthetic_input_output_pairs:\n",
    "        messages = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": output\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(messages) + \"\\n\")\n",
    "\n",
    "print(f\"Synthetic data saved to {synthetic_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb7e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1/10\n",
      "Sampling 2/10\n",
      "Sampling 3/10\n",
      "Sampling 4/10\n",
      "Sampling 5/10\n",
      "Sampling 6/10\n",
      "Sampling 7/10\n",
      "Sampling 8/10\n",
      "Sampling 9/10\n",
      "Sampling 10/10\n",
      "Input:  Write an email asking a colleague to make an introduction to Lumen, an energy finance company, to invite them to speak on a fireside chat event on green financethat I am hosting for Stanford Climate Week. Include information about Stanford Climate Week and the event details.\n",
      "==================================================\n",
      "Expected Output:  Hi Shawn,\n",
      "\n",
      " Stanford Climate Week is hosting a fireside chat on reimagining climate finance during the week of October 20th. We'd love to invite Lumen to join alongside Crux's CEO (who has confirmed) to discuss the future of funding green solutions. If there’s interest and availability, please let me know. More about Stanford Climate Week: https://www.stanfordclimateweek.com/\n",
      "\n",
      " Thanks for your help, and let me know if I can answer any questions!\n",
      "\n",
      " Warmly, Eyrin\n",
      "==================================================\n",
      "Output:  Subject: Invitation to Speak at Stanford Climate Week Fireside Chat on Green Finance  \n",
      "\n",
      "Dear [Colleague's Name],  \n",
      "\n",
      "I hope this message finds you well!  \n",
      "\n",
      "I’m reaching out to invite you to participate in a fireside chat event I’m organizing as part of Stanford Climate Week — a dynamic and highly respected gathering that brings together leaders, innovators, and thinkers at the forefront of climate solutions.  \n",
      "\n",
      "**About Stanford Climate Week**:  \n",
      "This annual event, hosted by Stanford University’s School of Earth, Environment & Sustainability, celebrates cutting-edge progress in climate action and sustainable innovation. With a diverse audience of academics, policymakers, entrepreneurs, and industry leaders, it offers a unique platform for meaningful dialogue on the global transition to a low-carbon future. The event emphasizes actionable insights and real-world applications, making it an ideal setting for conversations on the role of financial innovation in advancing climate resilience.  \n",
      "\n",
      "**Event Details**:  \n",
      "- **Topic**: \"Financing the Green Transition: Opportunities and Challenges in Energy Markets\"  \n",
      "- **Format**: Fireside chat (interactive, informal conversation with a small audience of approximately 60 attendees)  \n",
      "- **Date**: [Insert Date, e.g., October 25, 2024]  \n",
      "- **Time**: [Insert Time, e.g., 1:00 PM – 2:30 PM]  \n",
      "- **Location**: [Insert Venue, e.g., Stanford’s Ginzberg Hall or virtual via Zoom]  \n",
      "- **Target Audience**: Climate policy experts, energy finance professionals, sustainable investors, and university students  \n",
      "\n",
      "I’m particularly interested in your perspective as a thought leader in energy finance — especially how Lumen’s work at the intersection of renewable projects and financial markets can shape a more equitable and scalable green transition. A conversation featuring your insights would be incredibly valuable to the audience and would likely resonate with the themes of innovation, risk management, and investment in clean energy that are central to the week.  \n",
      "\n",
      "Would you be open to a brief 15–20 minute talk or informal dialogue at the event? If so, I’d be delighted to have you speak — and I’d be happy to help coordinate logistics, share promotional materials, and ensure your points are well-integrated into the conversation.  \n",
      "\n",
      "As a colleague, I know you’re deeply engaged in forward-thinking energy finance, and I believe your voice would contribute significantly to this important conversation. If you’re interested, I’d be glad to follow up with more details or schedule a quick call to discuss the topic further.  \n",
      "\n",
      "Thank you so much for your time and continued support — I’d be honored to have you join us.  \n",
      "\n",
      "Warm regards,  \n",
      "[Your Full Name]  \n",
      "[Your Job Title]  \n",
      "[Your Department or Affiliation]  \n",
      "Stanford University / [Your Organization, if applicable]  \n",
      "[Your Email Address]  \n",
      "[Optional: Phone Number]  \n",
      "\n",
      "P.S. For those attending, Stanford Climate Week will also feature panel discussions, workshops, and networking sessions — all designed to foster cross-sector collaboration. This fireside chat will be a highlight of the program, and your involvement would help elevate the conversation on green finance.  \n",
      "\n",
      "---  \n",
      "*Note: Be sure to replace bracketed placeholders with actual information before sending.*<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Launch the checkpoint for sampling\n",
    "# After running SFT with scripts/sft.py, we will see output like this:\n",
    "# tinker_cookbook.checkpoint_utils:75 [INFO] Saved checkpoints: {'state_path': 'tinker://61ac731e-53e2-43de-b76f-ab1fa1c6b0cc/weights/final', 'sampler_path': 'tinker://61ac731e-53e2-43de-b76f-ab1fa1c6b0cc/sampler_weights/final'}\n",
    "# The link after \"sampler_path\" is the model_path we will use below.\n",
    "\n",
    "sft_model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "sft_model_path = \"tinker://bb390d8c-8ab6-4d53-91ae-bd054debf79f/sampler_weights/final\"  # TODO: add your model path here\n",
    "\n",
    "sft_model_sampler = TinkerSampler(\n",
    "    model_name=sft_model_name,\n",
    "    model_path=sft_model_path,\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "sft_model_outputs = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [\n",
    "        renderers.Message(role=\"user\", content=input_text),\n",
    "    ]\n",
    "    output = await sft_model_sampler.generate(messages)\n",
    "    sft_model_outputs.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", sft_model_outputs[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", sft_model_outputs[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", sft_model_outputs[0][\"output\"])\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/sft_model_outputs.json\", \"w\") as f:\n",
    "    json.dump(sft_model_outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf33f28",
   "metadata": {},
   "source": [
    "**Analyze the Pros & Cons of Personalization SFT w/ Synthetic Data. (5 points)**\n",
    "\n",
    "Add your answer in writeup.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa56c28",
   "metadata": {},
   "source": [
    "### Method 3: Reinforcement Learning (60 points total)\n",
    "\n",
    "<img src=\"assets/rl.jpg\" alt=\"RL\" width=\"800\">\n",
    "\n",
    "In HW1, you experimented with one approach to RL-based personalization by labeling your own preference pairs and training the model with DPO. Here, we introduce **RLAIF (Reinforcement Learning from AI Feedback)**, a method that replaces human preference labels with AI-generated feedback. Instead of manually comparing outputs, we train a reward model to automatically evaluate which outputs better match our criteria. This dramatically reduces human labeling effort while still enabling preference-based learning.\n",
    "\n",
    "#### Step 1: Create a Reward Function using LLM-as-a-Judge\n",
    "\n",
    "For subjective tasks like email generation, a typical approach is to use an LLM-as-a-judge as the reward function. We will use the [pairwise preference collection](https://huggingface.co/datasets/prometheus-eval/Preference-Collection) from [Prometheus Eval](https://github.com/prometheus-eval/prometheus-eval). Unlike other pairwise preference datasets, Prometheus Eval includes explicit rubrics that ground the preference judgments. This is particularly suitable for personalization tasks, since personalized preferences may differ from general preferences—and we can specify our personalized requirements directly in the rubric.\n",
    "\n",
    "**Step 1.1 - Train the reward model (10 points):** We have defined preference data types in `rubric_preference_types.py` (**you DON'T need to change it**). Train the reward model based on `Qwen/Qwen3-30B-A3B-Instruct-2507` using `train_rubric_rm.py` by running `python -m scripts.train_rubric_rm {other arguments}` under the root directory.\n",
    "\n",
    "⚠️ **Important:** Training the reward model takes over 1 hour, as the dataset contains 199,760 instances. We strongly recommend running the script in [tmux](https://tmuxcheatsheet.com/) to ensure your job continues running even if you disconnect. **Don't leave this step until the last minute!**\n",
    "\n",
    "**Step 1.2 - Design and validate your rubric (10 points):** Design a rubric to evaluate your personalized emails. Use your trained reward model to compare the baseline output and SFT checkpoint output. Verify that the reward model's judgments align with your own preferences. If the reward model performs poorly, consider adjusting your rubric or returning to Step 1.1 to tune hyperparameters. \n",
    "\n",
    "*Note: The Tinker API uses LoRA for parameter-efficient fine-tuning. If you're interested in learning more about tuning hyperparameters in LoRA setups, check out this [blog post](https://thinkingmachines.ai/blog/lora/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21ad22de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling is paused for tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final. Reason: concurrent LoRA rate limit hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling is paused for tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final. Reason: concurrent LoRA rate limit hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling is paused for tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final. Reason: concurrent LoRA rate limit hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling is paused for tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final. Reason: concurrent LoRA rate limit hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling is paused for tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final. Reason: concurrent LoRA rate limit hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling is paused for tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final. Reason: concurrent LoRA rate limit hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 7/10\n",
      "Evaluating 8/10\n",
      "Evaluating 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling is paused for tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final. Reason: concurrent LoRA rate limit hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10/10\n",
      "Baseline preferred: 5\n",
      "SFT preferred: 5\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1: After training, you will see output like this:\n",
    "# tinker_cookbook.checkpoint_utils:75 [INFO] Saved checkpoints: {'state_path': 'tinker://aaef6db5-0e20-41ec-8023-7df145aa30b8/weights/final', 'sampler_path': 'tinker://aaef6db5-0e20-41ec-8023-7df145aa30b8/sampler_weights/final'}\n",
    "\n",
    "# Step 1.2: Design rubric to evaluate personalized email.\n",
    "\n",
    "from rubric_preference_types import PrometheusEvalComparisonRendererFromChatRenderer, PrometheusEvalComparison\n",
    "\n",
    "rubric = \"\"\"\n",
    "1) Task fit\n",
    "- Directly answers the prompt. Includes all requested parts.\n",
    "\n",
    "2) Tone and brevity\n",
    "- Friendly, warm, and professional/academic (not overly formal).\n",
    "- Concise: Most emails should be 50-150 words. Introduction emails should be 30-75 words.\n",
    "- No slang; appropriate salutations/closings.\n",
    "- Limited exclamations.\n",
    "\n",
    "3) Context incorporation\n",
    "- Provides details (roles, orgs, events) accurately.\n",
    "- Provides just enough context for the recipient to understand and act.\n",
    "\n",
    "4) Clarity and structure\n",
    "- Clear, scannable paragraphs (avoid excessive bullet points unless requested).\n",
    "- Well organized and easy to follow. No run-on sentences.\n",
    "\n",
    "5) Ready-to-send quality\n",
    "- Includes a clear call to action or next step when appropriate.\n",
    "\n",
    "Penalty criteria (reduce the score):\n",
    "- Exceeds recommended word count\n",
    "- Overly formal for the context\n",
    "- Provides unnecessary background or preamble when brevity is key\n",
    "\n",
    "Hard failure modes (score 0 regardless of above):\n",
    "- Fabricates facts not in prompt.\n",
    "- Shares private data that was not provided.\n",
    "- Offensive language.\n",
    "\"\"\"\n",
    "\n",
    "rm_model_name = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    "rm_model_path = \"tinker://b16c478c-8c12-4dba-af97-bf50fd3b4dd8/sampler_weights/final\"  # TODO: add your model path here\n",
    "\n",
    "grader_sampler = TinkerSampler(\n",
    "    model_name=rm_model_name,\n",
    "    model_path=rm_model_path,\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    "    \n",
    ")\n",
    "pairwise_renderer = PrometheusEvalComparisonRendererFromChatRenderer(convo_renderer=grader_sampler.renderer)\n",
    "\n",
    "baseline_results = json.load(open(\"results/baseline_results.json\"))\n",
    "sft_model_outputs = json.load(open(\"results/sft_model_outputs.json\"))\n",
    "\n",
    "ai_graded_preference = []\n",
    "for i, (baseline_result, sft_model_result) in enumerate(zip(baseline_results, sft_model_outputs)):\n",
    "    print(f\"Evaluating {i+1}/{len(baseline_results)}\")\n",
    "    prompt = baseline_result[\"input\"]\n",
    "    baseline_output = baseline_result[\"output\"]\n",
    "    sft_output = sft_model_result[\"output\"]\n",
    "    \n",
    "    messages = pairwise_renderer._comparison_to_convo(\n",
    "        PrometheusEvalComparison(\n",
    "            prompt_conversation=[renderers.Message(role=\"user\", content=prompt)],\n",
    "            completion_A=[renderers.Message(role=\"assistant\", content=baseline_output)],\n",
    "            completion_B=[renderers.Message(role=\"assistant\", content=sft_output)],\n",
    "            rubric=rubric,\n",
    "            reference=None\n",
    "        )\n",
    "    )\n",
    "    response = await grader_sampler.generate(messages)\n",
    "    response = str(response[\"content\"]).strip(\"<|endoftext|>\").strip()\n",
    "    preference = 1 if \"[RESULT] A\" in response else (-1 if \"[RESULT] B\" in response else 0)\n",
    "    ai_graded_preference.append({\n",
    "        \"input\": prompt,\n",
    "        \"baseline_output\": baseline_output,\n",
    "        \"sft_output\": sft_output,\n",
    "        \"preference\": preference,\n",
    "        \"grader_response\": response\n",
    "    })\n",
    "\n",
    "print(f\"Baseline preferred: {sum(1 for r in ai_graded_preference if r['preference'] == 1)}\")\n",
    "print(f\"SFT preferred: {sum(1 for r in ai_graded_preference if r['preference'] == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d13ef1",
   "metadata": {},
   "source": [
    "#### Step 2: Synthesize More Prompts\n",
    "\n",
    "In RLAIF, we don't need to provide ground truth outputs for input prompts. Similar to Method 2, synthesize additional inputs and split them into train and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "512a9dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Collected 10 inputs\n",
      "Write an email to your dentist requesting to reschedule your upcoming appointment due to a sudden conflict. Politely explain the situation and ask for available alternatives in the next two weeks.\n",
      "Write an email to a venue manager to inquire about availability and pricing for hosting a large-scale social event aimed at university students and young professionals.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: synthesize more prompts\n",
    "# Note #1: If you have chosen to use your data source, you may need to modify the data synthesis prompt accordingly.\n",
    "# Note #2: This cell takes around 5 minutes to run.\n",
    "import random\n",
    "\n",
    "async def synthesize_more_inputs_threadpool(run_count: int):\n",
    "    def sync_synthesize_inputs():\n",
    "        try:\n",
    "            selected_seed = random.choice(SEEDS)\n",
    "            selected_example_dict = random.choice(demonstrations)\n",
    "            selected_example = f\"Input: {selected_example_dict['input']}\\nOutput: {selected_example_dict['output']}\"\n",
    "            messages = [renderers.Message(\n",
    "                role=\"user\",\n",
    "                content=SYNTHESIZE_INPUT_PROMPT.replace(\"<examples>\", selected_example).replace(\"<seed>\", selected_seed)\n",
    "                )\n",
    "            ]\n",
    "            output = asyncio.run(data_synthesis_sampler.generate(messages))\n",
    "            return collect_synthetic_input(str(output[\"content\"]))\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:\n",
    "        futures = [executor.submit(sync_synthesize_inputs) for _ in range(run_count)]\n",
    "        results = []\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                print(f\"Collected {len(result)} inputs\")\n",
    "                results.extend(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "more_synthetic_inputs = await synthesize_more_inputs_threadpool(200)\n",
    "print(more_synthetic_inputs[0])\n",
    "print(more_synthetic_inputs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59b19493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 train data saved to results/rl_train_data.jsonl\n",
      "500 dev data saved to results/rl_dev_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "rl_data = {\n",
    "    \"train\": {\n",
    "        \"data\": more_synthetic_inputs[:-500],\n",
    "        \"output_file\": \"results/rl_train_data.jsonl\"\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"data\": more_synthetic_inputs[-500:],\n",
    "        \"output_file\": \"results/rl_dev_data.jsonl\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    with open(rl_data[split][\"output_file\"], \"w\") as f:\n",
    "        for prompt in rl_data[split][\"data\"]:\n",
    "            if split == \"train\":\n",
    "                # Add the system prompt to give the policy a better prior in RL training\n",
    "                prompt_conversation = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            else:\n",
    "                prompt_conversation = [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            d = {\n",
    "                \"prompt_conversation\": prompt_conversation,\n",
    "                \"reference\": None,\n",
    "                \"rubric\": rubric\n",
    "            }\n",
    "            f.write(json.dumps(d) + \"\\n\")\n",
    "    print(f\"{len(rl_data[split]['data'])} {split} data saved to {rl_data[split]['output_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4c3db",
   "metadata": {},
   "source": [
    "#### Step 3: Run the RLAIF Loop (20 points)\n",
    "\n",
    "**Complete rubric_preference_env.py to define the RL logic. (10 points)**\n",
    "\n",
    "For a given prompt $p$, the RLAIF procedure operates as follows:\n",
    "1. **Sample Generation**: The current policy samples `group_size` outputs $o_1, \\ldots, o_g$.\n",
    "2. **Pairwise Tournament**: We employ a tournament structure to compare each pair of outputs within the group using the rubric-based reward model (RM).\n",
    "3. **Scoring**: For each pairwise comparison, the winning output receives a score of 1, while the losing output receives a score of -1.\n",
    "4. **Reward Aggregation**: The final reward for a sample $o_i$ is its accumulated score across all tournament matches.\n",
    "\n",
    "Note: Even though you don't need to change `rubric_preference_types.py`, we suggest you reading it carefully as this will help you complete `rubric_preference_env.py`.\n",
    "\n",
    "\n",
    "**Complete rl_with_rubric_rm.py by writing an evaluator to monitor the model behavior change during the training stage. (10 points)** \n",
    "\n",
    "We use the reward model to compare the output from the initial model checkpoint and the output from the current checkpoint to see whether the policy is generating more personalized outputs as the RL run goes. You can monitor the dev set reward curve and look at the actual policy output to tune hyperparameters.\n",
    "\n",
    "Launch the training by running `python -m scripts.rl_with_rubric_rm {other parameters}` under the root directory.\n",
    "\n",
    "**Add the wandb link for your run in writeup.md (10 points).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5fd1f",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluate the Checkpoint\n",
    "\n",
    "Use your inference code to sample outputs from the RL checkpoint and evaluate the personalization quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8bc83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1/10\n",
      "Sampling 2/10\n",
      "Sampling 3/10\n",
      "Sampling 4/10\n",
      "Sampling 5/10\n",
      "Sampling 6/10\n",
      "Sampling 7/10\n",
      "Sampling 8/10\n",
      "Sampling 9/10\n",
      "Sampling 10/10\n",
      "Input:  Write an email asking a colleague to make an introduction to Lumen, an energy finance company, to invite them to speak on a fireside chat event on green financethat I am hosting for Stanford Climate Week. Include information about Stanford Climate Week and the event details.\n",
      "==================================================\n",
      "Expected Output:  Hi Shawn,\n",
      "\n",
      " Stanford Climate Week is hosting a fireside chat on reimagining climate finance during the week of October 20th. We'd love to invite Lumen to join alongside Crux's CEO (who has confirmed) to discuss the future of funding green solutions. If there’s interest and availability, please let me know. More about Stanford Climate Week: https://www.stanfordclimateweek.com/\n",
      "\n",
      " Thanks for your help, and let me know if I can answer any questions!\n",
      "\n",
      " Warmly, Eyrin\n",
      "==================================================\n",
      "Output:  Subject: Invitation to Speak at Stanford Climate Week Fireside Chat on Green Finance  \n",
      "\n",
      "Dear [Colleague's Name],  \n",
      "\n",
      "I hope this message finds you well! I’m reaching out to invite you to participate in a fireside chat event I’m hosting as part of **Stanford Climate Week**, a premier gathering of global leaders, innovators, and practitioners shaping the future of climate action and sustainable finance.  \n",
      "\n",
      "**About Stanford Climate Week**:  \n",
      "Hosted annually by Stanford’s School of Earth, Energy & Environmental Sciences, Stanford Climate Week brings together thought leaders from government, industry, academia, and civil society to explore the most pressing climate challenges and transformative solutions. With over 1,000 attendees from more than 40 countries, it’s one of the most influential climate events on the global calendar. This year’s theme—*“Scaling Green Finance for a Decarbonized Future”*—focuses on the critical role of capital markets, policy innovation, and financial mechanisms in accelerating clean energy transitions.  \n",
      "\n",
      "**Event Details**:  \n",
      "- **Event Title**: *Fireside Chat: “Driving Green Finance with Innovation and Scale”*  \n",
      "- **Date**: November 14, 2024  \n",
      "- **Time**: 5:30–6:30 PM (Pacific Time)  \n",
      "- **Location**: Stanford’s Haas Campus, Room 202, Baskin Building (virtual option available for global participants)  \n",
      "- **Audience**: Investors, policymakers, climate finance professionals, academic researchers, and sustainability leaders  \n",
      "\n",
      "Given your deep expertise in energy finance—particularly your work with Lumen, a dynamic leader in sustainable infrastructure and clean energy funding—I’d be honored to have you share your insights on how innovative financial instruments are de-risking and scaling green projects across sectors. Your perspective on long-term energy transitions, investor engagement, and project financing would be invaluable in sparking meaningful dialogue with our diverse audience.  \n",
      "\n",
      "I’d be grateful if you could help introduce Lumen as a pioneering force in the green finance ecosystem and consider joining the fireside chat as a guest speaker. I’ll handle all logistical arrangements, including coordinating with the Stanford organizing team, managing engagement with attendees, and providing promotional support.  \n",
      "\n",
      "If you’re interested, I’d be happy to schedule a brief call next week to walk through the event flow, discuss your speaking points, and answer any questions. Alternatively, I can also facilitate a short introduction with a pre-recorded video message if you prefer.  \n",
      "\n",
      "This is a unique opportunity to connect with a global audience deeply engaged in the practical realities of climate finance—and to position Lumen as a key influencer in this evolving space. I believe your voice would resonate strongly with our attendees, many of whom are actively shaping the next generation of green financial products and policy frameworks.  \n",
      "\n",
      "Thank you so much for considering this invitation. I’d be thrilled to have you join us at Stanford Climate Week and help inspire a more equitable and resilient future through bold financial innovation.  \n",
      "\n",
      "Warm regards,  \n",
      "[Your Full Name]  \n",
      "[Your Position or Title]  \n",
      "[Your Department or Affiliation, if applicable]  \n",
      "Stanford University – Climate & Energy Initiative  \n",
      "Email: [your.email@example.com]  \n",
      "Phone: [your phone number, optional]  \n",
      "\n",
      "P.S. The fireside chat will be livestreamed and archived on Stanford’s Climate Week platform—great visibility for Lumen and our broader mission to advance green finance innovation. Let me know what works best for your schedule!  \n",
      "\n",
      "---  \n",
      "*Note: Replace bracketed details with your personal and event-specific information before sending.*<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Launch the checkpoint for sampling\n",
    "# After running RLAIF with tinker_scripts/sft.py, we will see output like this:\n",
    "# tinker_cookbook.checkpoint_utils:75 [INFO] Saved checkpoints: {'state_path': 'tinker://24b18c15-0234-4bc0-9bd3-58eba5bfc210/weights/final', 'sampler_path': 'tinker://24b18c15-0234-4bc0-9bd3-58eba5bfc210/sampler_weights/final'}\n",
    "# The link after \"sampler_path\" is the model_path we will use below.\n",
    "\n",
    "rl_model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "rl_model_path = \"tinker://5aa0a181-292e-4320-abca-24d77fd861fc/sampler_weights/final\"  # TODO: add your model path here\n",
    "\n",
    "rl_model_sampler = TinkerSampler(\n",
    "    model_name=rl_model_name,\n",
    "    model_path=rl_model_path,\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "rl_model_outputs = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [\n",
    "        renderers.Message(role=\"user\", content=input_text),\n",
    "    ]\n",
    "    output = await rl_model_sampler.generate(messages)\n",
    "    rl_model_outputs.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", rl_model_outputs[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", rl_model_outputs[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", rl_model_outputs[0][\"output\"])\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/rl_model_outputs.json\", \"w\") as f:\n",
    "    json.dump(rl_model_outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe559f9",
   "metadata": {},
   "source": [
    "**Analyze the Pros & Cons of Personalization with RLAIF (10 points)**\n",
    "\n",
    "Provide a thorough analysis that addresses the following questions:\n",
    "\n",
    "1. Did you observe any reward hacking behavior where the policy produces outputs that score highly according to the reward model but don't actually match your personal preferences? Provide specific examples if observed. \n",
    "   \n",
    "   *For more background on reward hacking, we recommend reading this [blog post](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/).*\n",
    "\n",
    "2. Describe the approaches you tried to improve the results. What worked well? What didn't? Consider discussing rubric refinements, hyperparameter adjustments, or prompt engineering changes.\n",
    "\n",
    "3. Based on your experiments, what inherent limitations did you identify with RLAIF-based personalization? Consider factors such as data efficiency, scalability, alignment quality, or the challenge of specifying preferences through rubrics.\n",
    "\n",
    "**Add your answer in writeup.md**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49940844",
   "metadata": {},
   "source": [
    "### Extension: Demonstration-Iterated Task Optimization (DITTO) (20 points)\n",
    "\n",
    "> IMPORTANT: This section is **optional** for students taking CS329X for 3-units, and **mandatory** for those taking CS329X for 4-units.\n",
    "\n",
    "\n",
    "Congratulations on completing the main assignment! You've explored three personalization approaches: prompt engineering, supervised fine-tuning with synthetic data, and reinforcement learning. However, developing a trustworthy reward model remains challenging, which has motivated approaches like Direct Preference Optimization (DPO) that eliminate the need for explicit reward models. The primary challenge with applying DPO to personalization lies in obtaining sufficient pairwise comparisons from individual users—as you may have experienced in HW1, the labeling process can be tedious and mentally demanding.\n",
    "\n",
    "Our recent work [1] introduces **DITTO (Demonstration-Iterated Task Optimization)**, which addresses this limitation by efficiently generating online comparison data. DITTO treats users' demonstrations as preferred examples and contrasts them against outputs from the base LLM and its intermediate training checkpoints, thereby creating the necessary preference pairs without extensive manual labeling.\n",
    "\n",
    "<img src=\"assets/ditto.jpg\" alt=\"DITTO\" width=\"800\">\n",
    "\n",
    "**Your task:** Implement DITTO (Algorithm 1 in the paper) using Tinker in `scripts/ditto_dpo.py` and launch a training run with your collected seed data. Compare DITTO's performance against the three methods you implemented earlier (prompt engineering, SFT with synthetic data, and RLAIF). In your analysis, discuss:\n",
    "- How does DITTO's performance compare to other methods?\n",
    "- What advantages does DITTO offer in terms of data efficiency and ease of use?\n",
    "- Are there any limitations or trade-offs you observed?\n",
    "\n",
    "**Add your answer in writeup.md**\n",
    "\n",
    "**Hints:**\n",
    "1. **Implementation Scope:** You only need to implement the DPO component. For the initialization step \"$\\pi_0 \\leftarrow \\text{SFT}(\\pi_{\\text{ref}}, \\mathcal{D}_{E}), t = 0$\", use `scripts/sft.py` you completed previously. When configuring the SFT step, follow the paper's guidance on hyperparameters: \"For a dataset, we train with SFT until BCE train loss on a given batch approaches 1.00 (early stopping); ideally, we want an LLM to not overfit entirely to demos before DPO.\"\n",
    "2. **Core Algorithm:** The key innovation of DITTO is constructing preference pairs dynamically during training. As detailed in Section 3.2 (\"A Practical Algorithm\"), implement the following preference pair distribution:\n",
    " - 70% on-policy comparisons (i.e., ground truth v.s. current policy output)\n",
    " - 20% replay comparisons (i.e., ground truth v.s. old checkpoint output)\n",
    " - 10% intermodel comparisons (i.e., new checkpooint output v.s. old checkpoint output)\n",
    " Your main task is implementing this data construction logic. The DPO update mechanism itself is standardized and available through the [official implementation](https://github.com/thinking-machines-lab/tinker-cookbook/blob/main/tinker_cookbook/preference/train_dpo.py) in Tinker Cookbook.\n",
    "3. **Training and Evaluation:** As its name implied, DITTO is designed for data efficiency, so train using only the 10 demonstrations you collected initially. When evaluating DITTO against your earlier implementations, ensure fair comparison by testing on new, previously unseen inputs.\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] Shaikh, O., Lam, M. S., Hejna, J., Shao, Y., Cho, H., Bernstein, M. S., & Yang, D. (2024). [Aligning Language Models with Demonstrated Feedback](https://arxiv.org/pdf/2406.00888). *ICLR 2025*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c0b63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
