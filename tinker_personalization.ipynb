{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b87ac2",
   "metadata": {},
   "source": [
    "## HW2: Deep Personalization\n",
    "\n",
    "In the previous homework, you experimented with alignment methods including few-shot prompting, instruction tuning, RLHF, and DPO. Those experiments used general chat data to align a pre-trained model into a generally helpful assistant.\n",
    "\n",
    "However, helpfulness is often subjective. For example, while RLHF models tend to produce longer outputs, you may prefer short and concise responses when generating emails. In this assignment, you'll explore machine learning techniques to personalize model behavior to individual preferences.\n",
    "\n",
    "### Overview of LLM Personalization\n",
    "\n",
    "LLM Personalization encompasses the methodical application of machine learning techniques to customize large language model behavior based on user-specific data.\n",
    "\n",
    "<img src=\"assets/overview.jpg\" alt=\"Overview\" width=\"800\">\n",
    "\n",
    "A classic methodology is supervised fine-tuning (SFT) from CS224N‚Äîremember the sonnet generation assignment? üôÇ This is a widely used method especially before ChatGPT.\n",
    "\n",
    "<img src=\"assets/sft.jpg\" alt=\"SFT\" width=\"800\">\n",
    "\n",
    "The problem of the SFT method is that it requires the user to label desired outputs. This process is not only time-consuming but often infeasible for non-technical users. In this assignment, you'll explore alternative personalization methods that reduce this human effort barrier. Through hands-on experimentation, you'll analyze the pros and cons of different approaches.\n",
    "\n",
    "You will use **Tinker**, the cutting-edge training API developed by Thinking Machine Labs for this assignment. Besides following this handout, **we highly recommend checking out [Tinker Cookbook](https://tinker-docs.thinkingmachines.ai/) to understand Tinker abstraction**.\n",
    "\n",
    "**Note that Tinker handles the heavy computation for forward and backward passes. As a result, the remaining code can run on your laptop even during model training.**\n",
    "\n",
    "You also need to use [wandb](https://wandb.ai/site) to monitor the training process. Create an account if you don't have one yet, and make sure to add your API key to the `.env` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66a3c0",
   "metadata": {},
   "source": [
    "### Setup & Data Collection\n",
    "\n",
    "1. **Install Tinker**: Run the following cells to install the Tinker API.\n",
    "\n",
    "2. **Collect your personalized data**: Make a copy of `data.example.json` and rename this file to `data.json`. Add 10 data points in the format `{\"input\": \"...\", \"output\": \"...\"}` for email generation. The outputs should come from your own emails to better reflect your personal writing style.\n",
    "  \n",
    "    **Recall the lecture on \"Data, Data, and Data\", the similar logic applies here. Please ensure the emails you choose are representative to your style and cover diverse topic. Ensure the \"input\" instruction provides all necessary context (e.g., name, intent), similar to your request to LLM when you want to use LLM to get your things done.**\n",
    "   \n",
    "   - **Bonus (10 points):** While we expect most students to work on the email generation task, we encourage you to explore creative data sources for personalization. If you choose to collect `{\"input\": \"...\", \"output\": \"...\"}` pairs from a different domain (not email generation), you may need to modify some parts of the provided code below.\n",
    "   \n",
    "   - **If you're attempting the bonus**, describe your data source here:\n",
    "     \n",
    "     **TODO: Add your answer**\n",
    "\n",
    "Hint: You may want to use a programmatic approach to convert inputs and outputs into the required JSON format, which will help you avoid the hassle of dealing with \\n escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aca901",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb\n",
    "%pip install tinker\n",
    "%pip install git+https://github.com/thinking-machines-lab/tinker-cookbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c9e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import tinker\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40523e-edaa-4a5a-beea-41de12716491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- meta-llama/Llama-3.1-70B\n",
      "- meta-llama/Llama-3.1-8B\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.2-1B\n",
      "- meta-llama/Llama-3.2-3B\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "- Qwen/Qwen3-30B-A3B\n",
      "- Qwen/Qwen3-30B-A3B-Base\n",
      "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "- Qwen/Qwen3-32B\n",
      "- Qwen/Qwen3-4B-Instruct-2507\n",
      "- Qwen/Qwen3-8B\n",
      "- Qwen/Qwen3-8B-Base\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "service_client = tinker.ServiceClient()\n",
    "print(\"Available models:\")\n",
    "for item in service_client.get_server_capabilities().supported_models:\n",
    "    print(\"- \" + item.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c880275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 demonstrations loaded.\n"
     ]
    }
   ],
   "source": [
    "demonstrations = json.load(open(\"data.json\"))\n",
    "print(len(demonstrations), \"demonstrations loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb379b",
   "metadata": {},
   "source": [
    "### Get Baseline Results (10 points total)\n",
    "\n",
    "**Implement inference code (10 points)**\n",
    "\n",
    "In the following experiments, we will use `Qwen/Qwen3-4B-Instruct-2507` as the baseline policy. After finishing the assignment, we highly recommend trying out larger models available on Tinker.\n",
    "\n",
    "Use the inference code to obtain baseline results by prompting `Qwen/Qwen3-4B-Instruct-2507` with your inputs directly. `Qwen/Qwen3-4B-Instruct-2507` is a model that has been aligned with general human feedback. Examine its outputs to see whether they satisfy your personal preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinker import types\n",
    "from tinker_cookbook import renderers\n",
    "from tinker_cookbook.model_info import get_recommended_renderer_name\n",
    "from tinker_cookbook.tokenizer_utils import get_tokenizer\n",
    "\n",
    "\n",
    "class TinkerSampler():\n",
    "    \"\"\"A simple wrapper around Tinker ServiceClient to do sampling.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_path: str | None = None,  # tinker://..., obtained from Tinker training job\n",
    "        temperature: float = 0.9,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        top_k=-1,  # -1 means no limit\n",
    "    ):\n",
    "        tokenizer = get_tokenizer(model_name)\n",
    "        renderer_name = get_recommended_renderer_name(model_name)\n",
    "        # Read https://tinker-docs.thinkingmachines.ai/rendering to understand what renderer is\n",
    "        self.renderer = renderers.get_renderer(name=renderer_name, tokenizer=tokenizer)\n",
    "        self.sampling_params = types.SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=self.renderer.get_stop_sequences(),\n",
    "        )\n",
    "        self.sampling_client = service_client.create_sampling_client(\n",
    "            model_path=model_path,\n",
    "            base_model=model_name,\n",
    "        )\n",
    "        \n",
    "    async def generate(self, messages: list[renderers.Message]) -> renderers.Message:\n",
    "        # TODO: add your code here (10 points)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30445938",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sampler = TinkerSampler(\n",
    "    model_name=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    ")\n",
    "\n",
    "baseline_results = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [renderers.Message(role=\"user\", content=input_text)]\n",
    "    output = await baseline_sampler.generate(messages)\n",
    "    baseline_results.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", baseline_results[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", baseline_results[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", baseline_results[0][\"output\"])\n",
    "\n",
    "# The cell only prints the first example. Read through all baseline results in results/baseline_results.json\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/baseline_results.json\", \"w\") as f:\n",
    "    json.dump(baseline_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8637c22",
   "metadata": {},
   "source": [
    "### Method 1: Prompt Engineering (10 points total)\n",
    "\n",
    "<img src=\"assets/prompting.jpg\" alt=\"Prompting\" width=\"800\">\n",
    "\n",
    "Prompt engineering requires no model training. Instead, you craft instructions that guide the model to produce outputs matching your preferences by describing your desired behavior or providing in-context examples.\n",
    "\n",
    "If you need to refresh your understanding of prompt engineering techniques (such as few-shot examples, chain-of-thought prompting, etc.), we recommend revisiting HW1.\n",
    "\n",
    "**Engineer a system prompt (5 points)**\n",
    "\n",
    "Craft a system prompt that guides the model to generate emails in your personal style (or for your own selected task). Experiment with different prompt strategies to see which produces the most personalized outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "TODO: add your system prompt here\n",
    "\"\"\"\n",
    "\n",
    "# Explicitly require the model to only output the answer without any extra text\n",
    "system_prompt += \"\\n\\nMake sure to follow the instructions carefully and do not output anything else (such as \\\"Sure! Here's ...\\\", \\\"If you want ...\\\").\"\n",
    "\n",
    "prompt_engineering_results = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [\n",
    "        renderers.Message(role=\"system\", content=system_prompt),\n",
    "        renderers.Message(role=\"user\", content=input_text),\n",
    "    ]\n",
    "    output = await baseline_sampler.generate(messages)\n",
    "    prompt_engineering_results.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", prompt_engineering_results[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", prompt_engineering_results[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", prompt_engineering_results[0][\"output\"])\n",
    "\n",
    "# The cell only prints the first example. Read through all prompt engineering results in results/prompt_engineering_results.json\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/prompt_engineering_results.json\", \"w\") as f:\n",
    "    json.dump(prompt_engineering_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc7184",
   "metadata": {},
   "source": [
    "**Analyze the Pros & Cons of Personalization with Prompting. (5 points)**\n",
    "\n",
    "Give your answer in writeup.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178ba41",
   "metadata": {},
   "source": [
    "### Method 2: SFT with Synthetic Data (20 points total)\n",
    "\n",
    "<img src=\"assets/sft_with_synthetic_data.jpg\" alt=\"SFT with Synthetic Data\" width=\"800\">\n",
    "\n",
    "We can leverage more powerful models to synthesize training data for smaller models. This approach allows us to transfer the capabilities of large models into smaller, more efficient models without requiring manually labeled data. Even for your engineered prompt, larger models usually follow it better and they can generate high-quality outputs that serve as training targets for personalizing smaller models.\n",
    "\n",
    "**Step 1 - Synthesize inputs (5 points):** Use the provided code snippet to synthesize 100 input prompts similar to those in your collected data. Carefully review the quality of the generated prompts and adjust the synthesis parameters as needed before proceeding.\n",
    "\n",
    "*Note: If you're using a data source different from email generation (bonus track), you'll need to modify the code snippet accordingly.*\n",
    "\n",
    "**Step 2 - Synthesize outputs:** Now use the system prompt you engineered in Method 1 and a large LLM (e.g., `Qwen/Qwen3-235B-A22B-Instruct-2507`) to generate synthetic outputs for these input prompts. If the synthetic output quality is inadequate, consider implementing advanced techniques such as [chain-of-thought prompting](https://www.promptingguide.ai/techniques/cot), [self-critique](https://arxiv.org/abs/2305.11738), or other approaches that allocate more test-time compute to improve quality.\n",
    "\n",
    "Before proceeding, carefully review several samples to check data quality. Address any systematic issues you identify.\n",
    "\n",
    "**Step 3 - Train via SFT (15 points):** Complete `sft.py` to fine-tune `Qwen/Qwen3-4B-Instruct-2507` using supervised fine-tuning on your synthesized dataset.\n",
    "\n",
    "**Step 4 - Evaluate the checkpoint:** You may be surprised by how quickly training completes! Note that the model you trained is significantly smaller than the model used for data synthesis. How well can this smaller model generate personalized outputs? Use your inference code to sample outputs from the SFT checkpoint and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 (Synthesize inputs)\n",
    "# You may need to change SEED and SYNTHESIZE_INPUT_PROMPT if you choose your own task rather than email writing.\n",
    "\n",
    "SEEDS = [\n",
    "    \"research project communication\",\n",
    "    \"turn down request\",\n",
    "    \"open source outreach\",\n",
    "    \"job & career context\",\n",
    "    \"social events\",\n",
    "    \"time-sensitive crisis communications\",\n",
    "    \"reimbursement request\",\n",
    "    \"communication with health care providers (e.g., dentist, insurance companies, etc.)\",\n",
    "    \"legal & compliance\",\n",
    "    \"cold emailing\"\n",
    "]\n",
    "\n",
    "SYNTHESIZE_INPUT_PROMPT = \"\"\"\\\n",
    "Generate 10 new email writing instruction prompts based on the provided examples and the given seed. Give your answer in JSON format as follows. Do not output any other text.\n",
    "\n",
    "Output format:\n",
    "```json\n",
    "{\n",
    "    \"prompt_1\": \"Prompt 1\",\n",
    "    \"prompt_2\": \"Prompt 2\",\n",
    "    \"prompt_3\": \"Prompt 3\",\n",
    "    \"prompt_4\": \"Prompt 4\",\n",
    "    \"prompt_5\": \"Prompt 5\",\n",
    "    \"prompt_6\": \"Prompt 6\",\n",
    "    \"prompt_7\": \"Prompt 7\",\n",
    "    \"prompt_8\": \"Prompt 8\",\n",
    "    \"prompt_9\": \"Prompt 9\",\n",
    "    \"prompt_10\": \"Prompt 10\"\n",
    "}\n",
    "```\n",
    "\n",
    "----\n",
    "Examples:\n",
    "\n",
    "<examples>\n",
    "\n",
    "Seed: <seed>\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "data_synthesis_sampler = TinkerSampler(\n",
    "    model_name=\"Qwen/Qwen3-235B-A22B-Instruct-2507\",  # Use a stronger model for data synthesis\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "def collect_synthetic_input(output_str: str):\n",
    "    s = output_str.split(\"Output:\")[-1].strip(\"<|endoftext|>\").strip()\n",
    "    regex=r\"```(?:[a-zA-Z0-9_+-]*\\n)?([\\s\\S]*?)```\"\n",
    "    result = re.search(regex, s)\n",
    "    if result:\n",
    "        s = result.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"No JSON found in the output\")\n",
    "    result = json.loads(s)\n",
    "    return [result[f\"prompt_{i}\"] for i in range(1, 11)]\n",
    "\n",
    "examples = \"\\n\".join([\n",
    "    f\"Input: {d['input']}\\nOutput: {d['output']}\" for d in demonstrations[:3]\n",
    "])  # Use first 3 demonstrations as examples\n",
    "\n",
    "synthetic_inputs = []\n",
    "for seed in SEEDS:\n",
    "    print(f\"Collecting synthetic inputs for seed: {seed}\")\n",
    "    for _ in range(MAX_RETRIES):\n",
    "        messages = [renderers.Message(\n",
    "            role=\"user\",\n",
    "            content=SYNTHESIZE_INPUT_PROMPT.replace(\"<examples>\", examples).replace(\"<seed>\", seed)\n",
    "        )]\n",
    "        output = await data_synthesis_sampler.generate(messages)\n",
    "        try:\n",
    "            synthetic_inputs.extend(collect_synthetic_input(str(output[\"content\"])))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting synthetic input: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Collected {len(synthetic_inputs)} synthetic inputs\")\n",
    "print(\"Examples:\")\n",
    "print(synthetic_inputs[0])\n",
    "print(synthetic_inputs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd569953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 (Synthesize outputs)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 30\n",
    "\n",
    "async def process_all_prompts_threadpool(synthetic_inputs):\n",
    "    def sync_process_prompt(prompt):\n",
    "        try:\n",
    "            messages = [\n",
    "                renderers.Message(role=\"system\", content=system_prompt),  # Add the system prompt you have engineered that guides the model to output in the desired style\n",
    "                renderers.Message(role=\"user\", content=prompt)\n",
    "            ]\n",
    "            output = asyncio.run(data_synthesis_sampler.generate(messages))\n",
    "            return (prompt, output[\"content\"].strip(\"<|endoftext|>\").strip())\n",
    "        except Exception as e:\n",
    "            return (prompt, f\"ERROR: {str(e)}\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:\n",
    "        futures = [executor.submit(sync_process_prompt, prompt) for prompt in synthetic_inputs]\n",
    "        \n",
    "        results = []\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "    \n",
    "    return results\n",
    "    \n",
    "synthetic_input_output_pairs = await process_all_prompts_threadpool(synthetic_inputs)\n",
    "\n",
    "print(f\"\\nTotal successful pairs: {len(synthetic_input_output_pairs)}\")\n",
    "print(\"Examples:\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[0][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[0][1]}\\n\\n\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[-1][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[-1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb50481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix systematic issues in the synthetic data.\n",
    "# For example, if you find the model often outputs \"Sure! Here's ...\" at the beginning of the output, you can add code to remove that.\n",
    "# Or if you find the model forgets to include \"Best regards, [Your Name]\" at the end of the email, you can add code to append that.\n",
    "# Overall, it's a good practice to go through the data and improve its quality as you can.\n",
    "def fix_output(output: str):\n",
    "    # TODO: add your code here if needed\n",
    "    return output\n",
    "\n",
    "synthetic_input_output_pairs = [(prompt, fix_output(output)) for prompt, output in synthetic_input_output_pairs]\n",
    "\n",
    "print(\"Examples:\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[0][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[0][1]}\\n\\n\")\n",
    "print(f\"Prompt: {synthetic_input_output_pairs[-1][0]}\")\n",
    "print(f\"Output: {synthetic_input_output_pairs[-1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8bb85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data saved to results/synthetic_personalized_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "synthetic_data_path = \"results/synthetic_personalized_data.jsonl\"\n",
    "with open(synthetic_data_path, \"w\") as f:\n",
    "    for prompt, output in synthetic_input_output_pairs:\n",
    "        messages = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": output\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(messages) + \"\\n\")\n",
    "\n",
    "print(f\"Synthetic data saved to {synthetic_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280d651",
   "metadata": {},
   "source": [
    "**Step 3: SFT**\n",
    "\n",
    "Complete sft.py and train \"Qwen/Qwen3-4B-Instruct-2507\" with the synthetic data before you proceed to the next cell.\n",
    "\n",
    "Under the root directory, launch the training with `python -m scripts.sft {other arguments}`.\n",
    "\n",
    "The training takes a few minutes given we only synthesize a small number of training data. But you shall already be able to see the model behavior change!\n",
    "\n",
    "**Add the wandb link for your run in writeup.md (15 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Launch the checkpoint for sampling\n",
    "# After running SFT with scripts/sft.py, we will see output like this:\n",
    "# tinker_cookbook.checkpoint_utils:75 [INFO] Saved checkpoints: {'state_path': 'tinker://61ac731e-53e2-43de-b76f-ab1fa1c6b0cc/weights/final', 'sampler_path': 'tinker://61ac731e-53e2-43de-b76f-ab1fa1c6b0cc/sampler_weights/final'}\n",
    "# The link after \"sampler_path\" is the model_path we will use below.\n",
    "\n",
    "sft_model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "sft_model_path = \"tinker://\"  # TODO: add your model path here\n",
    "\n",
    "sft_model_sampler = TinkerSampler(\n",
    "    model_name=sft_model_name,\n",
    "    model_path=sft_model_path,\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "sft_model_outputs = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [\n",
    "        renderers.Message(role=\"user\", content=input_text),\n",
    "    ]\n",
    "    output = await sft_model_sampler.generate(messages)\n",
    "    sft_model_outputs.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", sft_model_outputs[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", sft_model_outputs[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", sft_model_outputs[0][\"output\"])\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/sft_model_outputs.json\", \"w\") as f:\n",
    "    json.dump(sft_model_outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf33f28",
   "metadata": {},
   "source": [
    "**Analyze the Pros & Cons of Personalization SFT w/ Synthetic Data. (5 points)**\n",
    "\n",
    "Add your answer in writeup.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa56c28",
   "metadata": {},
   "source": [
    "### Method 3: Reinforcement Learning (60 points total)\n",
    "\n",
    "<img src=\"assets/rl.jpg\" alt=\"RL\" width=\"800\">\n",
    "\n",
    "In HW1, you experimented with one approach to RL-based personalization by labeling your own preference pairs and training the model with DPO. Here, we introduce **RLAIF (Reinforcement Learning from AI Feedback)**, a method that replaces human preference labels with AI-generated feedback. Instead of manually comparing outputs, we train a reward model to automatically evaluate which outputs better match our criteria. This dramatically reduces human labeling effort while still enabling preference-based learning.\n",
    "\n",
    "#### Step 1: Create a Reward Function using LLM-as-a-Judge\n",
    "\n",
    "For subjective tasks like email generation, a typical approach is to use an LLM-as-a-judge as the reward function. We will use the [pairwise preference collection](https://huggingface.co/datasets/prometheus-eval/Preference-Collection) from [Prometheus Eval](https://github.com/prometheus-eval/prometheus-eval). Unlike other pairwise preference datasets, Prometheus Eval includes explicit rubrics that ground the preference judgments. This is particularly suitable for personalization tasks, since personalized preferences may differ from general preferences‚Äîand we can specify our personalized requirements directly in the rubric.\n",
    "\n",
    "**Step 1.1 - Train the reward model (10 points):** We have defined preference data types in `rubric_preference_types.py` (**you DON'T need to change it**). Train the reward model based on `Qwen/Qwen3-30B-A3B-Instruct-2507` using `train_rubric_rm.py` by running `python -m scripts.train_rubric_rm {other arguments}` under the root directory.\n",
    "\n",
    "‚ö†Ô∏è **Important:** Training the reward model takes over 1 hour, as the dataset contains 199,760 instances. We strongly recommend running the script in [tmux](https://tmuxcheatsheet.com/) to ensure your job continues running even if you disconnect. **Don't leave this step until the last minute!**\n",
    "\n",
    "**Step 1.2 - Design and validate your rubric (10 points):** Design a rubric to evaluate your personalized emails. Use your trained reward model to compare the baseline output and SFT checkpoint output. Verify that the reward model's judgments align with your own preferences. If the reward model performs poorly, consider adjusting your rubric or returning to Step 1.1 to tune hyperparameters. \n",
    "\n",
    "*Note: The Tinker API uses LoRA for parameter-efficient fine-tuning. If you're interested in learning more about tuning hyperparameters in LoRA setups, check out this [blog post](https://thinkingmachines.ai/blog/lora/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: After training, you will see output like this:\n",
    "# tinker_cookbook.checkpoint_utils:75 [INFO] Saved checkpoints: {'state_path': 'tinker://aaef6db5-0e20-41ec-8023-7df145aa30b8/weights/final', 'sampler_path': 'tinker://aaef6db5-0e20-41ec-8023-7df145aa30b8/sampler_weights/final'}\n",
    "\n",
    "# Step 1.2: Design rubric to evaluate personalized email.\n",
    "\n",
    "from rubric_preference_types import PrometheusEvalComparisonRendererFromChatRenderer, PrometheusEvalComparison\n",
    "\n",
    "rubric = \"\"\"\n",
    "TODO: add your personalized rubric here\n",
    "\"\"\"\n",
    "\n",
    "rm_model_name = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    "rm_model_path = \"tinker://\"  # TODO: add your model path here\n",
    "\n",
    "grader_sampler = TinkerSampler(\n",
    "    model_name=rm_model_name,\n",
    "    model_path=rm_model_path,\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "pairwise_renderer = PrometheusEvalComparisonRendererFromChatRenderer(convo_renderer=grader_sampler.renderer)\n",
    "\n",
    "baseline_results = json.load(open(\"results/baseline_results.json\"))\n",
    "sft_model_outputs = json.load(open(\"results/sft_model_outputs.json\"))\n",
    "\n",
    "ai_graded_preference = []\n",
    "for i, (baseline_result, sft_model_result) in enumerate(zip(baseline_results, sft_model_outputs)):\n",
    "    print(f\"Evaluating {i+1}/{len(baseline_results)}\")\n",
    "    prompt = baseline_result[\"input\"]\n",
    "    baseline_output = baseline_result[\"output\"]\n",
    "    sft_output = sft_model_result[\"output\"]\n",
    "    \n",
    "    messages = pairwise_renderer._comparison_to_convo(\n",
    "        PrometheusEvalComparison(\n",
    "            prompt_conversation=[renderers.Message(role=\"user\", content=prompt)],\n",
    "            completion_A=[renderers.Message(role=\"assistant\", content=baseline_output)],\n",
    "            completion_B=[renderers.Message(role=\"assistant\", content=sft_output)],\n",
    "            rubric=rubric,\n",
    "            reference=None\n",
    "        )\n",
    "    )\n",
    "    response = await grader_sampler.generate(messages)\n",
    "    response = str(response[\"content\"]).strip(\"<|endoftext|>\").strip()\n",
    "    preference = 1 if \"[RESULT] A\" in response else (-1 if \"[RESULT] B\" in response else 0)\n",
    "    ai_graded_preference.append({\n",
    "        \"input\": prompt,\n",
    "        \"baseline_output\": baseline_output,\n",
    "        \"sft_output\": sft_output,\n",
    "        \"preference\": preference,\n",
    "        \"grader_response\": response\n",
    "    })\n",
    "\n",
    "print(f\"Baseline preferred: {sum(1 for r in ai_graded_preference if r['preference'] == 1)}\")\n",
    "print(f\"SFT preferred: {sum(1 for r in ai_graded_preference if r['preference'] == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d13ef1",
   "metadata": {},
   "source": [
    "#### Step 2: Synthesize More Prompts\n",
    "\n",
    "In RLAIF, we don't need to provide ground truth outputs for input prompts. Similar to Method 2, synthesize additional inputs and split them into train and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: synthesize more prompts\n",
    "# Note #1: If you have chosen to use your data source, you may need to modify the data synthesis prompt accordingly.\n",
    "# Note #2: This cell takes around 5 minutes to run.\n",
    "import random\n",
    "\n",
    "async def synthesize_more_inputs_threadpool(run_count: int):\n",
    "    def sync_synthesize_inputs():\n",
    "        try:\n",
    "            selected_seed = random.choice(SEEDS)\n",
    "            selected_example_dict = random.choice(demonstrations)\n",
    "            selected_example = f\"Input: {selected_example_dict['input']}\\nOutput: {selected_example_dict['output']}\"\n",
    "            messages = [renderers.Message(\n",
    "                role=\"user\",\n",
    "                content=SYNTHESIZE_INPUT_PROMPT.replace(\"<examples>\", selected_example).replace(\"<seed>\", selected_seed)\n",
    "                )\n",
    "            ]\n",
    "            output = asyncio.run(data_synthesis_sampler.generate(messages))\n",
    "            return collect_synthetic_input(str(output[\"content\"]))\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:\n",
    "        futures = [executor.submit(sync_synthesize_inputs) for _ in range(run_count)]\n",
    "        results = []\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                print(f\"Collected {len(result)} inputs\")\n",
    "                results.extend(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "more_synthetic_inputs = await synthesize_more_inputs_threadpool(200)\n",
    "print(more_synthetic_inputs[0])\n",
    "print(more_synthetic_inputs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b19493",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_data = {\n",
    "    \"train\": {\n",
    "        \"data\": more_synthetic_inputs[:-500],\n",
    "        \"output_file\": \"results/rl_train_data.jsonl\"\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"data\": more_synthetic_inputs[-500:],\n",
    "        \"output_file\": \"results/rl_dev_data.jsonl\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    with open(rl_data[split][\"output_file\"], \"w\") as f:\n",
    "        for prompt in rl_data[split][\"data\"]:\n",
    "            if split == \"train\":\n",
    "                # Add the system prompt to give the policy a better prior in RL training\n",
    "                prompt_conversation = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            else:\n",
    "                prompt_conversation = [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            d = {\n",
    "                \"prompt_conversation\": prompt_conversation,\n",
    "                \"reference\": None,\n",
    "                \"rubric\": rubric\n",
    "            }\n",
    "            f.write(json.dumps(d) + \"\\n\")\n",
    "    print(f\"{len(rl_data[split]['data'])} {split} data saved to {rl_data[split]['output_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4c3db",
   "metadata": {},
   "source": [
    "#### Step 3: Run the RLAIF Loop (20 points)\n",
    "\n",
    "**Complete rubric_preference_env.py to define the RL logic. (10 points)**\n",
    "\n",
    "For a given prompt $p$, the RLAIF procedure operates as follows:\n",
    "1. **Sample Generation**: The current policy samples `group_size` outputs $o_1, \\ldots, o_g$.\n",
    "2. **Pairwise Tournament**: We employ a tournament structure to compare each pair of outputs within the group using the rubric-based reward model (RM).\n",
    "3. **Scoring**: For each pairwise comparison, the winning output receives a score of 1, while the losing output receives a score of -1.\n",
    "4. **Reward Aggregation**: The final reward for a sample $o_i$ is its accumulated score across all tournament matches.\n",
    "\n",
    "Note: Even though you don't need to change `rubric_preference_types.py`, we suggest you reading it carefully as this will help you complete `rubric_preference_env.py`.\n",
    "\n",
    "\n",
    "**Complete rl_with_rubric_rm.py by writing an evaluator to monitor the model behavior change during the training stage. (10 points)** \n",
    "\n",
    "We use the reward model to compare the output from the initial model checkpoint and the output from the current checkpoint to see whether the policy is generating more personalized outputs as the RL run goes. You can monitor the dev set reward curve and look at the actual policy output to tune hyperparameters.\n",
    "\n",
    "Launch the training by running `python -m scripts.rl_with_rubric_rm {other parameters}` under the root directory.\n",
    "\n",
    "**Add the wandb link for your run in writeup.md (10 points).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5fd1f",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluate the Checkpoint\n",
    "\n",
    "Use your inference code to sample outputs from the RL checkpoint and evaluate the personalization quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Launch the checkpoint for sampling\n",
    "# After running RLAIF with tinker_scripts/sft.py, we will see output like this:\n",
    "# tinker_cookbook.checkpoint_utils:75 [INFO] Saved checkpoints: {'state_path': 'tinker://24b18c15-0234-4bc0-9bd3-58eba5bfc210/weights/final', 'sampler_path': 'tinker://24b18c15-0234-4bc0-9bd3-58eba5bfc210/sampler_weights/final'}\n",
    "# The link after \"sampler_path\" is the model_path we will use below.\n",
    "\n",
    "rl_model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "rl_model_path = \"tinker://\"  # TODO: add your model path here\n",
    "\n",
    "rl_model_sampler = TinkerSampler(\n",
    "    model_name=rl_model_name,\n",
    "    model_path=rl_model_path,\n",
    "    temperature=1.0,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "rl_model_outputs = []\n",
    "for i, item in enumerate(demonstrations):\n",
    "    input_text, expected_output = item[\"input\"], item[\"output\"]\n",
    "    print(f\"Sampling {i+1}/{len(demonstrations)}\")\n",
    "    messages = [\n",
    "        renderers.Message(role=\"user\", content=input_text),\n",
    "    ]\n",
    "    output = await rl_model_sampler.generate(messages)\n",
    "    rl_model_outputs.append({\"input\": input_text, \"expected_output\": expected_output, \"output\": output[\"content\"]})\n",
    "\n",
    "print(\"Input: \", rl_model_outputs[0][\"input\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Expected Output: \", rl_model_outputs[0][\"expected_output\"])\n",
    "print((\"=\" * 50))\n",
    "print(\"Output: \", rl_model_outputs[0][\"output\"])\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/rl_model_outputs.json\", \"w\") as f:\n",
    "    json.dump(rl_model_outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe559f9",
   "metadata": {},
   "source": [
    "**Analyze the Pros & Cons of Personalization with RLAIF (10 points)**\n",
    "\n",
    "Provide a thorough analysis that addresses the following questions:\n",
    "\n",
    "1. Did you observe any reward hacking behavior where the policy produces outputs that score highly according to the reward model but don't actually match your personal preferences? Provide specific examples if observed. \n",
    "   \n",
    "   *For more background on reward hacking, we recommend reading this [blog post](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/).*\n",
    "\n",
    "2. Describe the approaches you tried to improve the results. What worked well? What didn't? Consider discussing rubric refinements, hyperparameter adjustments, or prompt engineering changes.\n",
    "\n",
    "3. Based on your experiments, what inherent limitations did you identify with RLAIF-based personalization? Consider factors such as data efficiency, scalability, alignment quality, or the challenge of specifying preferences through rubrics.\n",
    "\n",
    "**Add your answer in writeup.md**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49940844",
   "metadata": {},
   "source": [
    "### Bonus: Demonstration-Iterated Task Optimization (DITTO) (30 points)\n",
    "\n",
    "Congratulations on completing the main assignment! You've explored three personalization approaches: prompt engineering, supervised fine-tuning with synthetic data, and reinforcement learning. However, developing a trustworthy reward model remains challenging, which has motivated approaches like Direct Preference Optimization (DPO) that eliminate the need for explicit reward models. The primary challenge with applying DPO to personalization lies in obtaining sufficient pairwise comparisons from individual users‚Äîas you may have experienced in HW1, the labeling process can be tedious and mentally demanding.\n",
    "\n",
    "Our recent work [1] introduces **DITTO (Demonstration-Iterated Task Optimization)**, which addresses this limitation by efficiently generating online comparison data. DITTO treats users' demonstrations as preferred examples and contrasts them against outputs from the base LLM and its intermediate training checkpoints, thereby creating the necessary preference pairs without extensive manual labeling.\n",
    "\n",
    "<img src=\"assets/ditto.jpg\" alt=\"DITTO\" width=\"800\">\n",
    "\n",
    "**Your task:** Implement DITTO (Algorithm 1 in the paper) using Tinker in `scripts/ditto_dpo.py` and launch a training run with your collected seed data. Compare DITTO's performance against the three methods you implemented earlier (prompt engineering, SFT with synthetic data, and RLAIF). In your analysis, discuss:\n",
    "- How does DITTO's performance compare to other methods?\n",
    "- What advantages does DITTO offer in terms of data efficiency and ease of use?\n",
    "- Are there any limitations or trade-offs you observed?\n",
    "\n",
    "**Add your answer in writeup.md**\n",
    "\n",
    "**Hints:**\n",
    "1. **Implementation Scope:** You only need to implement the DPO component. For the initialization step \"$\\pi_0 \\leftarrow \\text{SFT}(\\pi_{\\text{ref}}, \\mathcal{D}_{E}), t = 0$\", use `scripts/sft.py` you completed previously. When configuring the SFT step, follow the paper's guidance on hyperparameters: \"For a dataset, we train with SFT until BCE train loss on a given batch approaches 1.00 (early stopping); ideally, we want an LLM to not overfit entirely to demos before DPO.\"\n",
    "2. **Core Algorithm:** The key innovation of DITTO is constructing preference pairs dynamically during training. As detailed in Section 3.2 (\"A Practical Algorithm\"), implement the following preference pair distribution:\n",
    " - 70% on-policy comparisons (i.e., ground truth v.s. current policy output)\n",
    " - 20% replay comparisons (i.e., ground truth v.s. old checkpoint output)\n",
    " - 10% intermodel comparisons (i.e., new checkpooint output v.s. old checkpoint output)\n",
    " Your main task is implementing this data construction logic. The DPO update mechanism itself is standardized and available through the [official implementation](https://github.com/thinking-machines-lab/tinker-cookbook/blob/main/tinker_cookbook/preference/train_dpo.py) in Tinker Cookbook.\n",
    "3. **Training and Evaluation:** As its name implied, DITTO is designed for data efficiency, so train using only the 10 demonstrations you collected initially. When evaluating DITTO against your earlier implementations, ensure fair comparison by testing on new, previously unseen inputs.\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] Shaikh, O., Lam, M. S., Hejna, J., Shao, Y., Cho, H., Bernstein, M. S., & Yang, D. (2024). [Aligning Language Models with Demonstrated Feedback](https://arxiv.org/pdf/2406.00888). *ICLR 2025*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c0b63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
